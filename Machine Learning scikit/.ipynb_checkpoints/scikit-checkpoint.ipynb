{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "# load_ : 크기작은거\n",
    "# fatch_ : 인터넷서 들고오는 큰데이터\n",
    "# make_ : 랜덤하게 \n",
    "# snake방식은 function이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'C:\\\\Users\\\\scminnoG15\\\\Anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy기반 -> Gpu지원 안한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bunch -> 데이터 묶음, 쪼갤 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'filename', 'target', 'target_names']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.DataFrame(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3\n",
       "0    5.1  3.5  1.4  0.2\n",
       "1    4.9  3.0  1.4  0.2\n",
       "2    4.7  3.2  1.3  0.2\n",
       "3    4.6  3.1  1.5  0.2\n",
       "4    5.0  3.6  1.4  0.2\n",
       "5    5.4  3.9  1.7  0.4\n",
       "6    4.6  3.4  1.4  0.3\n",
       "7    5.0  3.4  1.5  0.2\n",
       "8    4.4  2.9  1.4  0.2\n",
       "9    4.9  3.1  1.5  0.1\n",
       "10   5.4  3.7  1.5  0.2\n",
       "11   4.8  3.4  1.6  0.2\n",
       "12   4.8  3.0  1.4  0.1\n",
       "13   4.3  3.0  1.1  0.1\n",
       "14   5.8  4.0  1.2  0.2\n",
       "15   5.7  4.4  1.5  0.4\n",
       "16   5.4  3.9  1.3  0.4\n",
       "17   5.1  3.5  1.4  0.3\n",
       "18   5.7  3.8  1.7  0.3\n",
       "19   5.1  3.8  1.5  0.3\n",
       "20   5.4  3.4  1.7  0.2\n",
       "21   5.1  3.7  1.5  0.4\n",
       "22   4.6  3.6  1.0  0.2\n",
       "23   5.1  3.3  1.7  0.5\n",
       "24   4.8  3.4  1.9  0.2\n",
       "25   5.0  3.0  1.6  0.2\n",
       "26   5.0  3.4  1.6  0.4\n",
       "27   5.2  3.5  1.5  0.2\n",
       "28   5.2  3.4  1.4  0.2\n",
       "29   4.7  3.2  1.6  0.2\n",
       "..   ...  ...  ...  ...\n",
       "120  6.9  3.2  5.7  2.3\n",
       "121  5.6  2.8  4.9  2.0\n",
       "122  7.7  2.8  6.7  2.0\n",
       "123  6.3  2.7  4.9  1.8\n",
       "124  6.7  3.3  5.7  2.1\n",
       "125  7.2  3.2  6.0  1.8\n",
       "126  6.2  2.8  4.8  1.8\n",
       "127  6.1  3.0  4.9  1.8\n",
       "128  6.4  2.8  5.6  2.1\n",
       "129  7.2  3.0  5.8  1.6\n",
       "130  7.4  2.8  6.1  1.9\n",
       "131  7.9  3.8  6.4  2.0\n",
       "132  6.4  2.8  5.6  2.2\n",
       "133  6.3  2.8  5.1  1.5\n",
       "134  6.1  2.6  5.6  1.4\n",
       "135  7.7  3.0  6.1  2.3\n",
       "136  6.3  3.4  5.6  2.4\n",
       "137  6.4  3.1  5.5  1.8\n",
       "138  6.0  3.0  4.8  1.8\n",
       "139  6.9  3.1  5.4  2.1\n",
       "140  6.7  3.1  5.6  2.4\n",
       "141  6.9  3.1  5.1  2.3\n",
       "142  5.8  2.7  5.1  1.9\n",
       "143  6.8  3.2  5.9  2.3\n",
       "144  6.7  3.3  5.7  2.5\n",
       "145  6.7  3.0  5.2  2.3\n",
       "146  6.3  2.5  5.0  1.9\n",
       "147  6.5  3.0  5.2  2.0\n",
       "148  6.2  3.4  5.4  2.3\n",
       "149  5.9  3.0  5.1  1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dimension / 피쳐 -> row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.DataFrame(data.data, columns=data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "5                  5.4               3.9                1.7               0.4\n",
       "6                  4.6               3.4                1.4               0.3\n",
       "7                  5.0               3.4                1.5               0.2\n",
       "8                  4.4               2.9                1.4               0.2\n",
       "9                  4.9               3.1                1.5               0.1\n",
       "10                 5.4               3.7                1.5               0.2\n",
       "11                 4.8               3.4                1.6               0.2\n",
       "12                 4.8               3.0                1.4               0.1\n",
       "13                 4.3               3.0                1.1               0.1\n",
       "14                 5.8               4.0                1.2               0.2\n",
       "15                 5.7               4.4                1.5               0.4\n",
       "16                 5.4               3.9                1.3               0.4\n",
       "17                 5.1               3.5                1.4               0.3\n",
       "18                 5.7               3.8                1.7               0.3\n",
       "19                 5.1               3.8                1.5               0.3\n",
       "20                 5.4               3.4                1.7               0.2\n",
       "21                 5.1               3.7                1.5               0.4\n",
       "22                 4.6               3.6                1.0               0.2\n",
       "23                 5.1               3.3                1.7               0.5\n",
       "24                 4.8               3.4                1.9               0.2\n",
       "25                 5.0               3.0                1.6               0.2\n",
       "26                 5.0               3.4                1.6               0.4\n",
       "27                 5.2               3.5                1.5               0.2\n",
       "28                 5.2               3.4                1.4               0.2\n",
       "29                 4.7               3.2                1.6               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "120                6.9               3.2                5.7               2.3\n",
       "121                5.6               2.8                4.9               2.0\n",
       "122                7.7               2.8                6.7               2.0\n",
       "123                6.3               2.7                4.9               1.8\n",
       "124                6.7               3.3                5.7               2.1\n",
       "125                7.2               3.2                6.0               1.8\n",
       "126                6.2               2.8                4.8               1.8\n",
       "127                6.1               3.0                4.9               1.8\n",
       "128                6.4               2.8                5.6               2.1\n",
       "129                7.2               3.0                5.8               1.6\n",
       "130                7.4               2.8                6.1               1.9\n",
       "131                7.9               3.8                6.4               2.0\n",
       "132                6.4               2.8                5.6               2.2\n",
       "133                6.3               2.8                5.1               1.5\n",
       "134                6.1               2.6                5.6               1.4\n",
       "135                7.7               3.0                6.1               2.3\n",
       "136                6.3               3.4                5.6               2.4\n",
       "137                6.4               3.1                5.5               1.8\n",
       "138                6.0               3.0                4.8               1.8\n",
       "139                6.9               3.1                5.4               2.1\n",
       "140                6.7               3.1                5.6               2.4\n",
       "141                6.9               3.1                5.1               2.3\n",
       "142                5.8               2.7                5.1               1.9\n",
       "143                6.8               3.2                5.9               2.3\n",
       "144                6.7               3.3                5.7               2.5\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_target = pd.DataFrame(data.target,columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target\n",
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "5         0\n",
       "6         0\n",
       "7         0\n",
       "8         0\n",
       "9         0\n",
       "10        0\n",
       "11        0\n",
       "12        0\n",
       "13        0\n",
       "14        0\n",
       "15        0\n",
       "16        0\n",
       "17        0\n",
       "18        0\n",
       "19        0\n",
       "20        0\n",
       "21        0\n",
       "22        0\n",
       "23        0\n",
       "24        0\n",
       "25        0\n",
       "26        0\n",
       "27        0\n",
       "28        0\n",
       "29        0\n",
       "..      ...\n",
       "120       2\n",
       "121       2\n",
       "122       2\n",
       "123       2\n",
       "124       2\n",
       "125       2\n",
       "126       2\n",
       "127       2\n",
       "128       2\n",
       "129       2\n",
       "130       2\n",
       "131       2\n",
       "132       2\n",
       "133       2\n",
       "134       2\n",
       "135       2\n",
       "136       2\n",
       "137       2\n",
       "138       2\n",
       "139       2\n",
       "140       2\n",
       "141       2\n",
       "142       2\n",
       "143       2\n",
       "144       2\n",
       "145       2\n",
       "146       2\n",
       "147       2\n",
       "148       2\n",
       "149       2\n",
       "\n",
       "[150 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "붙이기\n",
    "\n",
    "merge는 컬럼이 같은게있을 때 join하는 것\n",
    "\n",
    "검증하는 방법??\n",
    "\n",
    "-> 현재 수집된 데이터는 모두 정답인 데이터라고 가정을 한다.\n",
    "\n",
    "-> iris데이터는 잘못된 데이터가 2개가 있다. DESC\n",
    "\n",
    "hold out 방식 : train set , test set(검증만) 분리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3총사 head tail sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.sample??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>6.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>6.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5.3</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.2</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "86                 6.7               3.1                4.7               1.5   \n",
       "121                5.6               2.8                4.9               2.0   \n",
       "133                6.3               2.8                5.1               1.5   \n",
       "79                 5.7               2.6                3.5               1.0   \n",
       "19                 5.1               3.8                1.5               0.3   \n",
       "137                6.4               3.1                5.5               1.8   \n",
       "132                6.4               2.8                5.6               2.2   \n",
       "15                 5.7               4.4                1.5               0.4   \n",
       "64                 5.6               2.9                3.6               1.3   \n",
       "58                 6.6               2.9                4.6               1.3   \n",
       "93                 5.0               2.3                3.3               1.0   \n",
       "90                 5.5               2.6                4.4               1.2   \n",
       "39                 5.1               3.4                1.5               0.2   \n",
       "115                6.4               3.2                5.3               2.3   \n",
       "78                 6.0               2.9                4.5               1.5   \n",
       "109                7.2               3.6                6.1               2.5   \n",
       "54                 6.5               2.8                4.6               1.5   \n",
       "29                 4.7               3.2                1.6               0.2   \n",
       "127                6.1               3.0                4.9               1.8   \n",
       "85                 6.0               3.4                4.5               1.6   \n",
       "12                 4.8               3.0                1.4               0.1   \n",
       "92                 5.8               2.6                4.0               1.2   \n",
       "36                 5.5               3.5                1.3               0.2   \n",
       "143                6.8               3.2                5.9               2.3   \n",
       "46                 5.1               3.8                1.6               0.2   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "8                  4.4               2.9                1.4               0.2   \n",
       "119                6.0               2.2                5.0               1.5   \n",
       "120                6.9               3.2                5.7               2.3   \n",
       "124                6.7               3.3                5.7               2.1   \n",
       "..                 ...               ...                ...               ...   \n",
       "97                 6.2               2.9                4.3               1.3   \n",
       "110                6.5               3.2                5.1               2.0   \n",
       "48                 5.3               3.7                1.5               0.2   \n",
       "113                5.7               2.5                5.0               2.0   \n",
       "134                6.1               2.6                5.6               1.4   \n",
       "77                 6.7               3.0                5.0               1.7   \n",
       "71                 6.1               2.8                4.0               1.3   \n",
       "111                6.4               2.7                5.3               1.9   \n",
       "82                 5.8               2.7                3.9               1.2   \n",
       "104                6.5               3.0                5.8               2.2   \n",
       "32                 5.2               4.1                1.5               0.1   \n",
       "80                 5.5               2.4                3.8               1.1   \n",
       "81                 5.5               2.4                3.7               1.0   \n",
       "122                7.7               2.8                6.7               2.0   \n",
       "38                 4.4               3.0                1.3               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "73                 6.1               2.8                4.7               1.2   \n",
       "135                7.7               3.0                6.1               2.3   \n",
       "123                6.3               2.7                4.9               1.8   \n",
       "91                 6.1               3.0                4.6               1.4   \n",
       "139                6.9               3.1                5.4               2.1   \n",
       "67                 5.8               2.7                4.1               1.0   \n",
       "112                6.8               3.0                5.5               2.1   \n",
       "68                 6.2               2.2                4.5               1.5   \n",
       "138                6.0               3.0                4.8               1.8   \n",
       "105                7.6               3.0                6.6               2.1   \n",
       "116                6.5               3.0                5.5               1.8   \n",
       "66                 5.6               3.0                4.5               1.5   \n",
       "65                 6.7               3.1                4.4               1.4   \n",
       "34                 4.9               3.1                1.5               0.2   \n",
       "\n",
       "     target  \n",
       "86        1  \n",
       "121       2  \n",
       "133       2  \n",
       "79        1  \n",
       "19        0  \n",
       "137       2  \n",
       "132       2  \n",
       "15        0  \n",
       "64        1  \n",
       "58        1  \n",
       "93        1  \n",
       "90        1  \n",
       "39        0  \n",
       "115       2  \n",
       "78        1  \n",
       "109       2  \n",
       "54        1  \n",
       "29        0  \n",
       "127       2  \n",
       "85        1  \n",
       "12        0  \n",
       "92        1  \n",
       "36        0  \n",
       "143       2  \n",
       "46        0  \n",
       "147       2  \n",
       "8         0  \n",
       "119       2  \n",
       "120       2  \n",
       "124       2  \n",
       "..      ...  \n",
       "97        1  \n",
       "110       2  \n",
       "48        0  \n",
       "113       2  \n",
       "134       2  \n",
       "77        1  \n",
       "71        1  \n",
       "111       2  \n",
       "82        1  \n",
       "104       2  \n",
       "32        0  \n",
       "80        1  \n",
       "81        1  \n",
       "122       2  \n",
       "38        0  \n",
       "1         0  \n",
       "73        1  \n",
       "135       2  \n",
       "123       2  \n",
       "91        1  \n",
       "139       2  \n",
       "67        1  \n",
       "112       2  \n",
       "68        1  \n",
       "138       2  \n",
       "105       2  \n",
       "116       2  \n",
       "66        1  \n",
       "65        1  \n",
       "34        0  \n",
       "\n",
       "[112 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.sample(frac=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = iris.sample(frac=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample : 중복을 제거하는 parameter가 부족하다.\n",
    "\n",
    "### 중복을 빼버리는 technique\n",
    "\n",
    "training set split하는 방법 1번째. : 텐서플로우 공홈 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.pop(train)\n",
    "# pop : list, dict literable에서 중요한 넘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.번째 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unpacking 해주는 놈\n",
    "\n",
    "x_train : pandas에서 잘라쓰는 넘. \n",
    "\n",
    "y_train : \n",
    "\n",
    "test_size :\n",
    "\n",
    "random_state : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(iris.iloc[:,:-1],\\\n",
    "                                                  iris.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 150개밖에 없는데 너무 많이 나누면 문제가 된다.\n",
    "\n",
    "data leakage문제가 일어난다.\n",
    "\n",
    "데이터가 적으면 정규분포가 안될 가능성이 높다.\n",
    "\n",
    "-> 랜덤하게 쪼갠다\n",
    "\n",
    "-> 맘에 안들면 random state를 정하고 실행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78     1\n",
       "139    2\n",
       "30     0\n",
       "105    2\n",
       "90     1\n",
       "66     1\n",
       "129    2\n",
       "108    2\n",
       "80     1\n",
       "63     1\n",
       "0      0\n",
       "110    2\n",
       "126    2\n",
       "148    2\n",
       "104    2\n",
       "9      0\n",
       "52     1\n",
       "14     0\n",
       "53     1\n",
       "31     0\n",
       "71     1\n",
       "10     0\n",
       "68     1\n",
       "84     1\n",
       "124    2\n",
       "20     0\n",
       "115    2\n",
       "141    2\n",
       "54     1\n",
       "21     0\n",
       "      ..\n",
       "101    2\n",
       "112    2\n",
       "11     0\n",
       "22     0\n",
       "74     1\n",
       "106    2\n",
       "40     0\n",
       "121    2\n",
       "13     0\n",
       "59     1\n",
       "12     0\n",
       "113    2\n",
       "81     1\n",
       "98     1\n",
       "103    2\n",
       "97     1\n",
       "87     1\n",
       "43     0\n",
       "56     1\n",
       "75     1\n",
       "146    2\n",
       "117    2\n",
       "86     1\n",
       "95     1\n",
       "91     1\n",
       "46     0\n",
       "144    2\n",
       "133    2\n",
       "60     1\n",
       "49     0\n",
       "Name: target, Length: 112, dtype: int32"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>6.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "13                 4.3               3.0                1.1               0.1\n",
       "120                6.9               3.2                5.7               2.3\n",
       "76                 6.8               2.8                4.8               1.4\n",
       "126                6.2               2.8                4.8               1.8\n",
       "74                 6.4               2.9                4.3               1.3\n",
       "46                 5.1               3.8                1.6               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "67                 5.8               2.7                4.1               1.0\n",
       "103                6.3               2.9                5.6               1.8\n",
       "62                 6.0               2.2                4.0               1.0\n",
       "106                4.9               2.5                4.5               1.7\n",
       "137                6.4               3.1                5.5               1.8\n",
       "134                6.1               2.6                5.6               1.4\n",
       "148                6.2               3.4                5.4               2.3\n",
       "122                7.7               2.8                6.7               2.0\n",
       "128                6.4               2.8                5.6               2.1\n",
       "8                  4.4               2.9                1.4               0.2\n",
       "83                 6.0               2.7                5.1               1.6\n",
       "129                7.2               3.0                5.8               1.6\n",
       "85                 6.0               3.4                4.5               1.6\n",
       "10                 5.4               3.7                1.5               0.2\n",
       "12                 4.8               3.0                1.4               0.1\n",
       "34                 4.9               3.1                1.5               0.2\n",
       "14                 5.8               4.0                1.2               0.2\n",
       "28                 5.2               3.4                1.4               0.2\n",
       "94                 5.6               2.7                4.2               1.3\n",
       "29                 4.7               3.2                1.6               0.2\n",
       "19                 5.1               3.8                1.5               0.3\n",
       "59                 5.2               2.7                3.9               1.4\n",
       "15                 5.7               4.4                1.5               0.4\n",
       "143                6.8               3.2                5.9               2.3\n",
       "136                6.3               3.4                5.6               2.4\n",
       "72                 6.3               2.5                4.9               1.5\n",
       "53                 5.5               2.3                4.0               1.3\n",
       "119                6.0               2.2                5.0               1.5\n",
       "49                 5.0               3.3                1.4               0.2\n",
       "135                7.7               3.0                6.1               2.3\n",
       "91                 6.1               3.0                4.6               1.4"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48     0\n",
       "127    2\n",
       "24     0\n",
       "131    2\n",
       "82     1\n",
       "108    2\n",
       "92     1\n",
       "115    2\n",
       "86     1\n",
       "6      0\n",
       "56     1\n",
       "43     0\n",
       "73     1\n",
       "113    2\n",
       "70     1\n",
       "102    2\n",
       "146    2\n",
       "63     1\n",
       "18     0\n",
       "117    2\n",
       "68     1\n",
       "69     1\n",
       "140    2\n",
       "145    2\n",
       "60     1\n",
       "61     1\n",
       "25     0\n",
       "5      0\n",
       "44     0\n",
       "125    2\n",
       "      ..\n",
       "36     0\n",
       "9      0\n",
       "42     0\n",
       "37     0\n",
       "1      0\n",
       "96     1\n",
       "107    2\n",
       "130    2\n",
       "97     1\n",
       "51     1\n",
       "118    2\n",
       "65     1\n",
       "31     0\n",
       "80     1\n",
       "35     0\n",
       "58     1\n",
       "132    2\n",
       "142    2\n",
       "124    2\n",
       "144    2\n",
       "39     0\n",
       "23     0\n",
       "104    2\n",
       "0      0\n",
       "89     1\n",
       "27     0\n",
       "99     1\n",
       "78     1\n",
       "66     1\n",
       "4      0\n",
       "Name: target, Length: 112, dtype: int32"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test set어째 만드는지 췌크해라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 0, 2, 2, 2, 0, 2, 1, 2, 1, 0, 2, 0, 0, 2, 0, 0, 1, 2, 1,\n",
       "       0, 1, 2, 1, 2, 1, 2, 0, 2, 2, 0, 0, 0, 0, 2, 1, 1, 2, 0, 1, 2, 0,\n",
       "       1, 2, 2, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 2, 0, 2, 1, 1, 1, 2, 0, 2, 2, 0, 1, 0, 2, 2, 2, 0,\n",
       "       2, 2, 0, 0, 1, 2, 2, 2, 2, 0, 0, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1,\n",
       "       0, 2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(knn.predict(X_test) == y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =\n",
    "knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 2 required positional arguments: 'X' and 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-b8442c39a88f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mknn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# n_neighbors=8일떄 성능 제일 좋다 하이퍼 파라미터이다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() missing 2 required positional arguments: 'X' and 'y'"
     ]
    }
   ],
   "source": [
    "# 추가\n",
    "for i in range(2,20):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    \n",
    "# n_neighbors=8일떄 성능 제일 좋다 하이퍼 파라미터이다.\n",
    "# 고수는 for문 안쓴다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~CV : Cross varidation 기법을 사용하겠다.\n",
    "\n",
    "데이터가 작을 때 cross varidation을 써서 성능을 예측한다.\n",
    "\n",
    "단점은 데이터가 많으면 오래 걸린다. 최종모델이 아니다.\n",
    "\n",
    "cross varidation을 사용\n",
    "\n",
    " 1. 대충의 성능이 어느정도 인가 check (평균으로 모델을 만들기 때문에)\n",
    " \n",
    " 2. 오버피팅인지 아닌지 체크하기위해서 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전의 score는 성능 췍크\n",
    "cross_val_score??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.93333333, 1.        , 1.        , 0.86666667,\n",
       "       0.93333333, 0.93333333, 1.        , 1.        , 1.        ])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(KNeighborsClassifier(), iris.iloc[:,:-1], iris.iloc[:,-1], cv=10,\n",
    "               n_jobs=-1)\n",
    "# 실무에선 10폴더로 쪼개서 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> data leakage , underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666668"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(KNeighborsClassifier(), iris.iloc[:,:-1], iris.iloc[:,-1], cv=10,\n",
    "               n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 작을때 : cross varidation을 사용해서 어느정도의 성능을 파악하자.\n",
    "    \n",
    "데이터가 클때 : training test split한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM은 수학적으로 구현했다. 속도가 느리다.\n",
    "\n",
    "커널 트릭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9666666666666668"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time \n",
    "np.mean(cross_val_score(KNeighborsClassifier(), iris.iloc[:,:-1], iris.iloc[:,-1], cv=10, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9800000000000001"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time \n",
    "np.mean(cross_val_score(SVC(), iris.iloc[:,:-1], iris.iloc[:,-1], cv=10, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors':range(1,10),'leaf_size':range(10,50,10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid)\n",
    "\n",
    "# 브루펄스방식? : 하나하나 체크한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scminnoG15\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\scminnoG15\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'leaf_size': [10, 20, 30, 40, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(iris.iloc[:,:-1],iris.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'leaf_size': 10, 'n_neighbors': 5}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scminnoG15\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\scminnoG15\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\scminnoG15\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\scminnoG15\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\scminnoG15\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00233181, 0.00149973, 0.00183336, 0.00166694, 0.00200518,\n",
       "        0.00183352, 0.00183304, 0.00166702, 0.00216277, 0.00216985,\n",
       "        0.00233356, 0.00216023, 0.0021673 , 0.00216516, 0.00200121,\n",
       "        0.00150013, 0.00149632, 0.00150005, 0.00133308, 0.00150013,\n",
       "        0.0016671 , 0.00166663, 0.00166734, 0.0014952 , 0.0016675 ,\n",
       "        0.00166686, 0.00133332, 0.00183622, 0.00183638, 0.00150029,\n",
       "        0.00150005, 0.00166678, 0.0020078 , 0.0018332 , 0.00183312,\n",
       "        0.001834  , 0.00116666, 0.00149949, 0.00166726, 0.0014991 ,\n",
       "        0.00150013, 0.00166059, 0.00233301, 0.00166591, 0.00199954]),\n",
       " 'std_fit_time': array([2.34392694e-04, 1.12391596e-07, 2.35966176e-04, 2.36078568e-04,\n",
       "        4.08665549e-04, 2.36078568e-04, 4.73845528e-04, 2.36359607e-04,\n",
       "        2.37029666e-04, 2.38049301e-04, 4.72101391e-04, 4.67181610e-04,\n",
       "        4.71933315e-04, 2.36306718e-04, 1.56139395e-06, 0.00000000e+00,\n",
       "        5.92059470e-06, 1.94992131e-06, 2.35572866e-04, 3.37174788e-07,\n",
       "        2.35797890e-04, 2.35123299e-04, 2.36640525e-04, 6.97099757e-06,\n",
       "        2.37033876e-04, 2.35966337e-04, 2.35572785e-04, 4.73450809e-04,\n",
       "        2.38270502e-04, 4.08510707e-04, 1.12391596e-07, 2.36191120e-04,\n",
       "        4.18279621e-04, 2.36190959e-04, 4.71426560e-04, 2.36415742e-04,\n",
       "        2.35460394e-04, 1.10692885e-06, 2.35348002e-04, 1.65563159e-06,\n",
       "        1.94667955e-07, 2.48617654e-04, 2.35572866e-04, 2.36810373e-04,\n",
       "        4.08510799e-04]),\n",
       " 'mean_score_time': array([0.0030009 , 0.00249879, 0.00233205, 0.00249894, 0.00266051,\n",
       "        0.00233173, 0.00266512, 0.00366696, 0.0026652 , 0.00367673,\n",
       "        0.00300097, 0.00299931, 0.0036668 , 0.00299931, 0.00383178,\n",
       "        0.00233261, 0.00249998, 0.00233261, 0.00216571, 0.00199922,\n",
       "        0.00199938, 0.00233285, 0.0020024 , 0.00199938, 0.00216556,\n",
       "        0.00216603, 0.00233253, 0.00232967, 0.00266353, 0.0024991 ,\n",
       "        0.00199922, 0.00266592, 0.00282486, 0.00283273, 0.00283329,\n",
       "        0.00283313, 0.00233221, 0.00233221, 0.00216548, 0.00216619,\n",
       "        0.00216579, 0.00316691, 0.00316628, 0.00283329, 0.00283249]),\n",
       " 'std_score_time': array([4.10165627e-04, 2.97360213e-07, 2.35910683e-04, 4.05233662e-07,\n",
       "        2.39646215e-04, 2.35516770e-04, 2.36136749e-04, 1.02704718e-03,\n",
       "        2.36588234e-04, 8.55205401e-04, 4.08123073e-04, 4.09776183e-04,\n",
       "        6.23458167e-04, 5.84003864e-07, 6.22651724e-04, 2.35966176e-04,\n",
       "        5.61957980e-07, 2.35461681e-04, 2.35404218e-04, 2.24783192e-07,\n",
       "        3.89335909e-07, 2.36134823e-04, 4.78555972e-06, 1.94667955e-07,\n",
       "        2.36191440e-04, 2.36022352e-04, 2.35741393e-04, 2.37260656e-04,\n",
       "        6.23396743e-04, 7.07561353e-04, 2.97360213e-07, 6.25178624e-04,\n",
       "        4.77310039e-04, 2.36415742e-04, 2.36641006e-04, 4.72719093e-04,\n",
       "        2.36191120e-04, 2.35853945e-04, 2.35741393e-04, 2.36078568e-04,\n",
       "        2.36359526e-04, 6.23755709e-04, 2.36809413e-04, 2.36977700e-04,\n",
       "        2.35235933e-04]),\n",
       " 'param_leaf_size': masked_array(data=[10, 10, 10, 10, 10, 10, 10, 10, 10, 20, 20, 20, 20, 20,\n",
       "                    20, 20, 20, 20, 30, 30, 30, 30, 30, 30, 30, 30, 30, 40,\n",
       "                    40, 40, 40, 40, 40, 40, 40, 40, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_neighbors': masked_array(data=[1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                    1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                    1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'leaf_size': 10, 'n_neighbors': 1},\n",
       "  {'leaf_size': 10, 'n_neighbors': 2},\n",
       "  {'leaf_size': 10, 'n_neighbors': 3},\n",
       "  {'leaf_size': 10, 'n_neighbors': 4},\n",
       "  {'leaf_size': 10, 'n_neighbors': 5},\n",
       "  {'leaf_size': 10, 'n_neighbors': 6},\n",
       "  {'leaf_size': 10, 'n_neighbors': 7},\n",
       "  {'leaf_size': 10, 'n_neighbors': 8},\n",
       "  {'leaf_size': 10, 'n_neighbors': 9},\n",
       "  {'leaf_size': 20, 'n_neighbors': 1},\n",
       "  {'leaf_size': 20, 'n_neighbors': 2},\n",
       "  {'leaf_size': 20, 'n_neighbors': 3},\n",
       "  {'leaf_size': 20, 'n_neighbors': 4},\n",
       "  {'leaf_size': 20, 'n_neighbors': 5},\n",
       "  {'leaf_size': 20, 'n_neighbors': 6},\n",
       "  {'leaf_size': 20, 'n_neighbors': 7},\n",
       "  {'leaf_size': 20, 'n_neighbors': 8},\n",
       "  {'leaf_size': 20, 'n_neighbors': 9},\n",
       "  {'leaf_size': 30, 'n_neighbors': 1},\n",
       "  {'leaf_size': 30, 'n_neighbors': 2},\n",
       "  {'leaf_size': 30, 'n_neighbors': 3},\n",
       "  {'leaf_size': 30, 'n_neighbors': 4},\n",
       "  {'leaf_size': 30, 'n_neighbors': 5},\n",
       "  {'leaf_size': 30, 'n_neighbors': 6},\n",
       "  {'leaf_size': 30, 'n_neighbors': 7},\n",
       "  {'leaf_size': 30, 'n_neighbors': 8},\n",
       "  {'leaf_size': 30, 'n_neighbors': 9},\n",
       "  {'leaf_size': 40, 'n_neighbors': 1},\n",
       "  {'leaf_size': 40, 'n_neighbors': 2},\n",
       "  {'leaf_size': 40, 'n_neighbors': 3},\n",
       "  {'leaf_size': 40, 'n_neighbors': 4},\n",
       "  {'leaf_size': 40, 'n_neighbors': 5},\n",
       "  {'leaf_size': 40, 'n_neighbors': 6},\n",
       "  {'leaf_size': 40, 'n_neighbors': 7},\n",
       "  {'leaf_size': 40, 'n_neighbors': 8},\n",
       "  {'leaf_size': 40, 'n_neighbors': 9},\n",
       "  {'leaf_size': 50, 'n_neighbors': 1},\n",
       "  {'leaf_size': 50, 'n_neighbors': 2},\n",
       "  {'leaf_size': 50, 'n_neighbors': 3},\n",
       "  {'leaf_size': 50, 'n_neighbors': 4},\n",
       "  {'leaf_size': 50, 'n_neighbors': 5},\n",
       "  {'leaf_size': 50, 'n_neighbors': 6},\n",
       "  {'leaf_size': 50, 'n_neighbors': 7},\n",
       "  {'leaf_size': 50, 'n_neighbors': 8},\n",
       "  {'leaf_size': 50, 'n_neighbors': 9}],\n",
       " 'split0_test_score': array([0.98039216, 0.96078431, 0.98039216, 0.98039216, 0.98039216,\n",
       "        0.98039216, 0.98039216, 0.98039216, 0.96078431, 0.98039216,\n",
       "        0.96078431, 0.98039216, 0.98039216, 0.98039216, 0.98039216,\n",
       "        0.98039216, 0.98039216, 0.96078431, 0.98039216, 0.96078431,\n",
       "        0.98039216, 0.98039216, 0.98039216, 0.98039216, 0.98039216,\n",
       "        0.98039216, 0.96078431, 0.98039216, 0.96078431, 0.98039216,\n",
       "        0.98039216, 0.98039216, 0.98039216, 0.98039216, 0.98039216,\n",
       "        0.96078431, 0.98039216, 0.96078431, 0.98039216, 0.98039216,\n",
       "        0.98039216, 0.98039216, 0.98039216, 0.98039216, 0.96078431]),\n",
       " 'split1_test_score': array([0.92156863, 0.94117647, 0.96078431, 0.96078431, 0.98039216,\n",
       "        0.96078431, 0.96078431, 0.98039216, 0.98039216, 0.92156863,\n",
       "        0.94117647, 0.96078431, 0.96078431, 0.98039216, 0.96078431,\n",
       "        0.96078431, 0.98039216, 0.98039216, 0.92156863, 0.94117647,\n",
       "        0.96078431, 0.96078431, 0.98039216, 0.96078431, 0.96078431,\n",
       "        0.98039216, 0.98039216, 0.92156863, 0.94117647, 0.96078431,\n",
       "        0.96078431, 0.98039216, 0.96078431, 0.96078431, 0.98039216,\n",
       "        0.98039216, 0.92156863, 0.94117647, 0.96078431, 0.96078431,\n",
       "        0.98039216, 0.96078431, 0.96078431, 0.98039216, 0.98039216]),\n",
       " 'split2_test_score': array([1.        , 0.95833333, 1.        , 0.97916667, 1.        ,\n",
       "        0.97916667, 0.97916667, 0.97916667, 0.97916667, 1.        ,\n",
       "        0.95833333, 1.        , 0.97916667, 1.        , 0.97916667,\n",
       "        0.97916667, 0.97916667, 0.97916667, 1.        , 0.95833333,\n",
       "        1.        , 0.97916667, 1.        , 0.97916667, 0.97916667,\n",
       "        0.97916667, 0.97916667, 1.        , 0.95833333, 1.        ,\n",
       "        0.97916667, 1.        , 0.97916667, 0.97916667, 0.97916667,\n",
       "        0.97916667, 1.        , 0.95833333, 1.        , 0.97916667,\n",
       "        1.        , 0.97916667, 0.97916667, 0.97916667, 0.97916667]),\n",
       " 'mean_test_score': array([0.96666667, 0.95333333, 0.98      , 0.97333333, 0.98666667,\n",
       "        0.97333333, 0.97333333, 0.98      , 0.97333333, 0.96666667,\n",
       "        0.95333333, 0.98      , 0.97333333, 0.98666667, 0.97333333,\n",
       "        0.97333333, 0.98      , 0.97333333, 0.96666667, 0.95333333,\n",
       "        0.98      , 0.97333333, 0.98666667, 0.97333333, 0.97333333,\n",
       "        0.98      , 0.97333333, 0.96666667, 0.95333333, 0.98      ,\n",
       "        0.97333333, 0.98666667, 0.97333333, 0.97333333, 0.98      ,\n",
       "        0.97333333, 0.96666667, 0.95333333, 0.98      , 0.97333333,\n",
       "        0.98666667, 0.97333333, 0.97333333, 0.98      , 0.97333333]),\n",
       " 'std_test_score': array([0.03333333, 0.00878204, 0.01592466, 0.00902067, 0.00914659,\n",
       "        0.00902067, 0.00902067, 0.00057166, 0.00902067, 0.03333333,\n",
       "        0.00878204, 0.01592466, 0.00902067, 0.00914659, 0.00902067,\n",
       "        0.00902067, 0.00057166, 0.00902067, 0.03333333, 0.00878204,\n",
       "        0.01592466, 0.00902067, 0.00914659, 0.00902067, 0.00902067,\n",
       "        0.00057166, 0.00902067, 0.03333333, 0.00878204, 0.01592466,\n",
       "        0.00902067, 0.00914659, 0.00902067, 0.00902067, 0.00057166,\n",
       "        0.00902067, 0.03333333, 0.00878204, 0.01592466, 0.00902067,\n",
       "        0.00914659, 0.00902067, 0.00902067, 0.00057166, 0.00902067]),\n",
       " 'rank_test_score': array([36, 41,  6, 16,  1, 16, 16,  6, 16, 36, 41,  6, 16,  1, 16, 16,  6,\n",
       "        16, 36, 41,  6, 16,  1, 16, 16,  6, 16, 36, 41,  6, 16,  1, 16, 16,\n",
       "         6, 16, 36, 41,  6, 16,  1, 16, 16,  6, 16]),\n",
       " 'split0_train_score': array([1.        , 0.96969697, 0.94949495, 0.94949495, 0.95959596,\n",
       "        0.95959596, 0.95959596, 0.96969697, 0.95959596, 1.        ,\n",
       "        0.96969697, 0.94949495, 0.94949495, 0.95959596, 0.95959596,\n",
       "        0.95959596, 0.96969697, 0.95959596, 1.        , 0.96969697,\n",
       "        0.94949495, 0.94949495, 0.95959596, 0.95959596, 0.95959596,\n",
       "        0.96969697, 0.95959596, 1.        , 0.96969697, 0.94949495,\n",
       "        0.94949495, 0.95959596, 0.95959596, 0.95959596, 0.96969697,\n",
       "        0.95959596, 1.        , 0.96969697, 0.94949495, 0.94949495,\n",
       "        0.95959596, 0.95959596, 0.95959596, 0.96969697, 0.95959596]),\n",
       " 'split1_train_score': array([1.        , 0.98989899, 0.98989899, 0.97979798, 0.97979798,\n",
       "        0.97979798, 0.98989899, 0.97979798, 0.97979798, 1.        ,\n",
       "        0.98989899, 0.98989899, 0.97979798, 0.97979798, 0.97979798,\n",
       "        0.98989899, 0.97979798, 0.97979798, 1.        , 0.98989899,\n",
       "        0.98989899, 0.97979798, 0.97979798, 0.97979798, 0.98989899,\n",
       "        0.97979798, 0.97979798, 1.        , 0.98989899, 0.98989899,\n",
       "        0.97979798, 0.97979798, 0.97979798, 0.98989899, 0.97979798,\n",
       "        0.97979798, 1.        , 0.98989899, 0.98989899, 0.97979798,\n",
       "        0.97979798, 0.97979798, 0.98989899, 0.97979798, 0.97979798]),\n",
       " 'split2_train_score': array([1.        , 0.97058824, 0.95098039, 0.96078431, 0.96078431,\n",
       "        0.96078431, 0.97058824, 0.96078431, 0.96078431, 1.        ,\n",
       "        0.97058824, 0.95098039, 0.96078431, 0.96078431, 0.96078431,\n",
       "        0.97058824, 0.96078431, 0.96078431, 1.        , 0.97058824,\n",
       "        0.95098039, 0.96078431, 0.96078431, 0.96078431, 0.97058824,\n",
       "        0.96078431, 0.96078431, 1.        , 0.97058824, 0.95098039,\n",
       "        0.96078431, 0.96078431, 0.96078431, 0.97058824, 0.96078431,\n",
       "        0.96078431, 1.        , 0.97058824, 0.95098039, 0.96078431,\n",
       "        0.96078431, 0.96078431, 0.97058824, 0.96078431, 0.96078431]),\n",
       " 'mean_train_score': array([1.        , 0.97672806, 0.96345811, 0.96335908, 0.96672608,\n",
       "        0.96672608, 0.97336106, 0.97009309, 0.96672608, 1.        ,\n",
       "        0.97672806, 0.96345811, 0.96335908, 0.96672608, 0.96672608,\n",
       "        0.97336106, 0.97009309, 0.96672608, 1.        , 0.97672806,\n",
       "        0.96345811, 0.96335908, 0.96672608, 0.96672608, 0.97336106,\n",
       "        0.97009309, 0.96672608, 1.        , 0.97672806, 0.96345811,\n",
       "        0.96335908, 0.96672608, 0.96672608, 0.97336106, 0.97009309,\n",
       "        0.96672608, 1.        , 0.97672806, 0.96345811, 0.96335908,\n",
       "        0.96672608, 0.96672608, 0.97336106, 0.97009309, 0.96672608]),\n",
       " 'std_train_score': array([0.        , 0.00932036, 0.01870636, 0.01250441, 0.00925595,\n",
       "        0.00925595, 0.01252557, 0.00776735, 0.00925595, 0.        ,\n",
       "        0.00932036, 0.01870636, 0.01250441, 0.00925595, 0.00925595,\n",
       "        0.01252557, 0.00776735, 0.00925595, 0.        , 0.00932036,\n",
       "        0.01870636, 0.01250441, 0.00925595, 0.00925595, 0.01252557,\n",
       "        0.00776735, 0.00925595, 0.        , 0.00932036, 0.01870636,\n",
       "        0.01250441, 0.00925595, 0.00925595, 0.01252557, 0.00776735,\n",
       "        0.00925595, 0.        , 0.00932036, 0.01870636, 0.01250441,\n",
       "        0.00925595, 0.00925595, 0.01252557, 0.00776735, 0.00925595])}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scminnoG15\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\scminnoG15\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\scminnoG15\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\scminnoG15\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\scminnoG15\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_leaf_size</th>\n",
       "      <th>param_n_neighbors</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002332</td>\n",
       "      <td>2.343927e-04</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>4.101656e-04</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 1}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>36</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.123916e-07</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>2.973602e-07</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 2}</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>41</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.976728</td>\n",
       "      <td>0.009320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001833</td>\n",
       "      <td>2.359662e-04</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>2.359107e-04</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 3}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.015925</td>\n",
       "      <td>6</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.950980</td>\n",
       "      <td>0.963458</td>\n",
       "      <td>0.018706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001667</td>\n",
       "      <td>2.360786e-04</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>4.052337e-07</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 4}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.963359</td>\n",
       "      <td>0.012504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002005</td>\n",
       "      <td>4.086655e-04</td>\n",
       "      <td>0.002661</td>\n",
       "      <td>2.396462e-04</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 5}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.009147</td>\n",
       "      <td>1</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.009256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001834</td>\n",
       "      <td>2.360786e-04</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>2.355168e-04</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 6}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.009256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001833</td>\n",
       "      <td>4.738455e-04</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>2.361367e-04</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 7}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.973361</td>\n",
       "      <td>0.012526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001667</td>\n",
       "      <td>2.363596e-04</td>\n",
       "      <td>0.003667</td>\n",
       "      <td>1.027047e-03</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 8}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>6</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.970093</td>\n",
       "      <td>0.007767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.002163</td>\n",
       "      <td>2.370297e-04</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>2.365882e-04</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 9}</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.009256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002170</td>\n",
       "      <td>2.380493e-04</td>\n",
       "      <td>0.003677</td>\n",
       "      <td>8.552054e-04</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'leaf_size': 20, 'n_neighbors': 1}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>36</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002334</td>\n",
       "      <td>4.721014e-04</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>4.081231e-04</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>{'leaf_size': 20, 'n_neighbors': 2}</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>41</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.976728</td>\n",
       "      <td>0.009320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.002160</td>\n",
       "      <td>4.671816e-04</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>4.097762e-04</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>{'leaf_size': 20, 'n_neighbors': 3}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.015925</td>\n",
       "      <td>6</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.950980</td>\n",
       "      <td>0.963458</td>\n",
       "      <td>0.018706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.002167</td>\n",
       "      <td>4.719333e-04</td>\n",
       "      <td>0.003667</td>\n",
       "      <td>6.234582e-04</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>{'leaf_size': 20, 'n_neighbors': 4}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.963359</td>\n",
       "      <td>0.012504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002165</td>\n",
       "      <td>2.363067e-04</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>5.840039e-07</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>{'leaf_size': 20, 'n_neighbors': 5}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.009147</td>\n",
       "      <td>1</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.009256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.002001</td>\n",
       "      <td>1.561394e-06</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>6.226517e-04</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>{'leaf_size': 20, 'n_neighbors': 6}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.009256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>2.359662e-04</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>{'leaf_size': 20, 'n_neighbors': 7}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.973361</td>\n",
       "      <td>0.012526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.001496</td>\n",
       "      <td>5.920595e-06</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>5.619580e-07</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>{'leaf_size': 20, 'n_neighbors': 8}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>6</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.970093</td>\n",
       "      <td>0.007767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.949921e-06</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>2.354617e-04</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>{'leaf_size': 20, 'n_neighbors': 9}</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.009256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001333</td>\n",
       "      <td>2.355729e-04</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>2.354042e-04</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>{'leaf_size': 30, 'n_neighbors': 1}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>36</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>3.371748e-07</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>2.247832e-07</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>{'leaf_size': 30, 'n_neighbors': 2}</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>41</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.976728</td>\n",
       "      <td>0.009320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.001667</td>\n",
       "      <td>2.357979e-04</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>3.893359e-07</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>{'leaf_size': 30, 'n_neighbors': 3}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.015925</td>\n",
       "      <td>6</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.950980</td>\n",
       "      <td>0.963458</td>\n",
       "      <td>0.018706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.001667</td>\n",
       "      <td>2.351233e-04</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>2.361348e-04</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>{'leaf_size': 30, 'n_neighbors': 4}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.963359</td>\n",
       "      <td>0.012504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.001667</td>\n",
       "      <td>2.366405e-04</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>4.785560e-06</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>{'leaf_size': 30, 'n_neighbors': 5}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.009147</td>\n",
       "      <td>1</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.009256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.001495</td>\n",
       "      <td>6.970998e-06</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>1.946680e-07</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>{'leaf_size': 30, 'n_neighbors': 6}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.009256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.001667</td>\n",
       "      <td>2.370339e-04</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>2.361914e-04</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>{'leaf_size': 30, 'n_neighbors': 7}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.973361</td>\n",
       "      <td>0.012526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.001667</td>\n",
       "      <td>2.359663e-04</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>2.360224e-04</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>{'leaf_size': 30, 'n_neighbors': 8}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>6</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.970093</td>\n",
       "      <td>0.007767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.001333</td>\n",
       "      <td>2.355728e-04</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>2.357414e-04</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>{'leaf_size': 30, 'n_neighbors': 9}</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.009256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.001836</td>\n",
       "      <td>4.734508e-04</td>\n",
       "      <td>0.002330</td>\n",
       "      <td>2.372607e-04</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>{'leaf_size': 40, 'n_neighbors': 1}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>36</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.001836</td>\n",
       "      <td>2.382705e-04</td>\n",
       "      <td>0.002664</td>\n",
       "      <td>6.233967e-04</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>{'leaf_size': 40, 'n_neighbors': 2}</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>41</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.976728</td>\n",
       "      <td>0.009320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>4.085107e-04</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>7.075614e-04</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>{'leaf_size': 40, 'n_neighbors': 3}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.015925</td>\n",
       "      <td>6</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.950980</td>\n",
       "      <td>0.963458</td>\n",
       "      <td>0.018706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.123916e-07</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>2.973602e-07</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>{'leaf_size': 40, 'n_neighbors': 4}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.963359</td>\n",
       "      <td>0.012504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.001667</td>\n",
       "      <td>2.361911e-04</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>6.251786e-04</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>{'leaf_size': 40, 'n_neighbors': 5}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.009147</td>\n",
       "      <td>1</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.009256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.002008</td>\n",
       "      <td>4.182796e-04</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>4.773100e-04</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>{'leaf_size': 40, 'n_neighbors': 6}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.009256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.001833</td>\n",
       "      <td>2.361910e-04</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>2.364157e-04</td>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "      <td>{'leaf_size': 40, 'n_neighbors': 7}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.973361</td>\n",
       "      <td>0.012526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.001833</td>\n",
       "      <td>4.714266e-04</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>2.366410e-04</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>{'leaf_size': 40, 'n_neighbors': 8}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>6</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.970093</td>\n",
       "      <td>0.007767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.001834</td>\n",
       "      <td>2.364157e-04</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>4.727191e-04</td>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "      <td>{'leaf_size': 40, 'n_neighbors': 9}</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.009256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.001167</td>\n",
       "      <td>2.354604e-04</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>2.361911e-04</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 1}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>36</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.001499</td>\n",
       "      <td>1.106929e-06</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>2.358539e-04</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 2}</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>41</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.976728</td>\n",
       "      <td>0.009320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.001667</td>\n",
       "      <td>2.353480e-04</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>2.357414e-04</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 3}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.015925</td>\n",
       "      <td>6</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.950980</td>\n",
       "      <td>0.963458</td>\n",
       "      <td>0.018706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.001499</td>\n",
       "      <td>1.655632e-06</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>2.360786e-04</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 4}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.963359</td>\n",
       "      <td>0.012504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.946680e-07</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>2.363595e-04</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 5}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.009147</td>\n",
       "      <td>1</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.009256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.001661</td>\n",
       "      <td>2.486177e-04</td>\n",
       "      <td>0.003167</td>\n",
       "      <td>6.237557e-04</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 6}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.009256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.002333</td>\n",
       "      <td>2.355729e-04</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>2.368094e-04</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 7}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.973361</td>\n",
       "      <td>0.012526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.001666</td>\n",
       "      <td>2.368104e-04</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>2.369777e-04</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 8}</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>6</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.970093</td>\n",
       "      <td>0.007767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.002000</td>\n",
       "      <td>4.085108e-04</td>\n",
       "      <td>0.002832</td>\n",
       "      <td>2.352359e-04</td>\n",
       "      <td>50</td>\n",
       "      <td>9</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 9}</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>16</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.009256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.002332  2.343927e-04         0.003001    4.101656e-04   \n",
       "1        0.001500  1.123916e-07         0.002499    2.973602e-07   \n",
       "2        0.001833  2.359662e-04         0.002332    2.359107e-04   \n",
       "3        0.001667  2.360786e-04         0.002499    4.052337e-07   \n",
       "4        0.002005  4.086655e-04         0.002661    2.396462e-04   \n",
       "5        0.001834  2.360786e-04         0.002332    2.355168e-04   \n",
       "6        0.001833  4.738455e-04         0.002665    2.361367e-04   \n",
       "7        0.001667  2.363596e-04         0.003667    1.027047e-03   \n",
       "8        0.002163  2.370297e-04         0.002665    2.365882e-04   \n",
       "9        0.002170  2.380493e-04         0.003677    8.552054e-04   \n",
       "10       0.002334  4.721014e-04         0.003001    4.081231e-04   \n",
       "11       0.002160  4.671816e-04         0.002999    4.097762e-04   \n",
       "12       0.002167  4.719333e-04         0.003667    6.234582e-04   \n",
       "13       0.002165  2.363067e-04         0.002999    5.840039e-07   \n",
       "14       0.002001  1.561394e-06         0.003832    6.226517e-04   \n",
       "15       0.001500  0.000000e+00         0.002333    2.359662e-04   \n",
       "16       0.001496  5.920595e-06         0.002500    5.619580e-07   \n",
       "17       0.001500  1.949921e-06         0.002333    2.354617e-04   \n",
       "18       0.001333  2.355729e-04         0.002166    2.354042e-04   \n",
       "19       0.001500  3.371748e-07         0.001999    2.247832e-07   \n",
       "20       0.001667  2.357979e-04         0.001999    3.893359e-07   \n",
       "21       0.001667  2.351233e-04         0.002333    2.361348e-04   \n",
       "22       0.001667  2.366405e-04         0.002002    4.785560e-06   \n",
       "23       0.001495  6.970998e-06         0.001999    1.946680e-07   \n",
       "24       0.001667  2.370339e-04         0.002166    2.361914e-04   \n",
       "25       0.001667  2.359663e-04         0.002166    2.360224e-04   \n",
       "26       0.001333  2.355728e-04         0.002333    2.357414e-04   \n",
       "27       0.001836  4.734508e-04         0.002330    2.372607e-04   \n",
       "28       0.001836  2.382705e-04         0.002664    6.233967e-04   \n",
       "29       0.001500  4.085107e-04         0.002499    7.075614e-04   \n",
       "30       0.001500  1.123916e-07         0.001999    2.973602e-07   \n",
       "31       0.001667  2.361911e-04         0.002666    6.251786e-04   \n",
       "32       0.002008  4.182796e-04         0.002825    4.773100e-04   \n",
       "33       0.001833  2.361910e-04         0.002833    2.364157e-04   \n",
       "34       0.001833  4.714266e-04         0.002833    2.366410e-04   \n",
       "35       0.001834  2.364157e-04         0.002833    4.727191e-04   \n",
       "36       0.001167  2.354604e-04         0.002332    2.361911e-04   \n",
       "37       0.001499  1.106929e-06         0.002332    2.358539e-04   \n",
       "38       0.001667  2.353480e-04         0.002165    2.357414e-04   \n",
       "39       0.001499  1.655632e-06         0.002166    2.360786e-04   \n",
       "40       0.001500  1.946680e-07         0.002166    2.363595e-04   \n",
       "41       0.001661  2.486177e-04         0.003167    6.237557e-04   \n",
       "42       0.002333  2.355729e-04         0.003166    2.368094e-04   \n",
       "43       0.001666  2.368104e-04         0.002833    2.369777e-04   \n",
       "44       0.002000  4.085108e-04         0.002832    2.352359e-04   \n",
       "\n",
       "   param_leaf_size param_n_neighbors                               params  \\\n",
       "0               10                 1  {'leaf_size': 10, 'n_neighbors': 1}   \n",
       "1               10                 2  {'leaf_size': 10, 'n_neighbors': 2}   \n",
       "2               10                 3  {'leaf_size': 10, 'n_neighbors': 3}   \n",
       "3               10                 4  {'leaf_size': 10, 'n_neighbors': 4}   \n",
       "4               10                 5  {'leaf_size': 10, 'n_neighbors': 5}   \n",
       "5               10                 6  {'leaf_size': 10, 'n_neighbors': 6}   \n",
       "6               10                 7  {'leaf_size': 10, 'n_neighbors': 7}   \n",
       "7               10                 8  {'leaf_size': 10, 'n_neighbors': 8}   \n",
       "8               10                 9  {'leaf_size': 10, 'n_neighbors': 9}   \n",
       "9               20                 1  {'leaf_size': 20, 'n_neighbors': 1}   \n",
       "10              20                 2  {'leaf_size': 20, 'n_neighbors': 2}   \n",
       "11              20                 3  {'leaf_size': 20, 'n_neighbors': 3}   \n",
       "12              20                 4  {'leaf_size': 20, 'n_neighbors': 4}   \n",
       "13              20                 5  {'leaf_size': 20, 'n_neighbors': 5}   \n",
       "14              20                 6  {'leaf_size': 20, 'n_neighbors': 6}   \n",
       "15              20                 7  {'leaf_size': 20, 'n_neighbors': 7}   \n",
       "16              20                 8  {'leaf_size': 20, 'n_neighbors': 8}   \n",
       "17              20                 9  {'leaf_size': 20, 'n_neighbors': 9}   \n",
       "18              30                 1  {'leaf_size': 30, 'n_neighbors': 1}   \n",
       "19              30                 2  {'leaf_size': 30, 'n_neighbors': 2}   \n",
       "20              30                 3  {'leaf_size': 30, 'n_neighbors': 3}   \n",
       "21              30                 4  {'leaf_size': 30, 'n_neighbors': 4}   \n",
       "22              30                 5  {'leaf_size': 30, 'n_neighbors': 5}   \n",
       "23              30                 6  {'leaf_size': 30, 'n_neighbors': 6}   \n",
       "24              30                 7  {'leaf_size': 30, 'n_neighbors': 7}   \n",
       "25              30                 8  {'leaf_size': 30, 'n_neighbors': 8}   \n",
       "26              30                 9  {'leaf_size': 30, 'n_neighbors': 9}   \n",
       "27              40                 1  {'leaf_size': 40, 'n_neighbors': 1}   \n",
       "28              40                 2  {'leaf_size': 40, 'n_neighbors': 2}   \n",
       "29              40                 3  {'leaf_size': 40, 'n_neighbors': 3}   \n",
       "30              40                 4  {'leaf_size': 40, 'n_neighbors': 4}   \n",
       "31              40                 5  {'leaf_size': 40, 'n_neighbors': 5}   \n",
       "32              40                 6  {'leaf_size': 40, 'n_neighbors': 6}   \n",
       "33              40                 7  {'leaf_size': 40, 'n_neighbors': 7}   \n",
       "34              40                 8  {'leaf_size': 40, 'n_neighbors': 8}   \n",
       "35              40                 9  {'leaf_size': 40, 'n_neighbors': 9}   \n",
       "36              50                 1  {'leaf_size': 50, 'n_neighbors': 1}   \n",
       "37              50                 2  {'leaf_size': 50, 'n_neighbors': 2}   \n",
       "38              50                 3  {'leaf_size': 50, 'n_neighbors': 3}   \n",
       "39              50                 4  {'leaf_size': 50, 'n_neighbors': 4}   \n",
       "40              50                 5  {'leaf_size': 50, 'n_neighbors': 5}   \n",
       "41              50                 6  {'leaf_size': 50, 'n_neighbors': 6}   \n",
       "42              50                 7  {'leaf_size': 50, 'n_neighbors': 7}   \n",
       "43              50                 8  {'leaf_size': 50, 'n_neighbors': 8}   \n",
       "44              50                 9  {'leaf_size': 50, 'n_neighbors': 9}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "0            0.980392           0.921569           1.000000         0.966667   \n",
       "1            0.960784           0.941176           0.958333         0.953333   \n",
       "2            0.980392           0.960784           1.000000         0.980000   \n",
       "3            0.980392           0.960784           0.979167         0.973333   \n",
       "4            0.980392           0.980392           1.000000         0.986667   \n",
       "5            0.980392           0.960784           0.979167         0.973333   \n",
       "6            0.980392           0.960784           0.979167         0.973333   \n",
       "7            0.980392           0.980392           0.979167         0.980000   \n",
       "8            0.960784           0.980392           0.979167         0.973333   \n",
       "9            0.980392           0.921569           1.000000         0.966667   \n",
       "10           0.960784           0.941176           0.958333         0.953333   \n",
       "11           0.980392           0.960784           1.000000         0.980000   \n",
       "12           0.980392           0.960784           0.979167         0.973333   \n",
       "13           0.980392           0.980392           1.000000         0.986667   \n",
       "14           0.980392           0.960784           0.979167         0.973333   \n",
       "15           0.980392           0.960784           0.979167         0.973333   \n",
       "16           0.980392           0.980392           0.979167         0.980000   \n",
       "17           0.960784           0.980392           0.979167         0.973333   \n",
       "18           0.980392           0.921569           1.000000         0.966667   \n",
       "19           0.960784           0.941176           0.958333         0.953333   \n",
       "20           0.980392           0.960784           1.000000         0.980000   \n",
       "21           0.980392           0.960784           0.979167         0.973333   \n",
       "22           0.980392           0.980392           1.000000         0.986667   \n",
       "23           0.980392           0.960784           0.979167         0.973333   \n",
       "24           0.980392           0.960784           0.979167         0.973333   \n",
       "25           0.980392           0.980392           0.979167         0.980000   \n",
       "26           0.960784           0.980392           0.979167         0.973333   \n",
       "27           0.980392           0.921569           1.000000         0.966667   \n",
       "28           0.960784           0.941176           0.958333         0.953333   \n",
       "29           0.980392           0.960784           1.000000         0.980000   \n",
       "30           0.980392           0.960784           0.979167         0.973333   \n",
       "31           0.980392           0.980392           1.000000         0.986667   \n",
       "32           0.980392           0.960784           0.979167         0.973333   \n",
       "33           0.980392           0.960784           0.979167         0.973333   \n",
       "34           0.980392           0.980392           0.979167         0.980000   \n",
       "35           0.960784           0.980392           0.979167         0.973333   \n",
       "36           0.980392           0.921569           1.000000         0.966667   \n",
       "37           0.960784           0.941176           0.958333         0.953333   \n",
       "38           0.980392           0.960784           1.000000         0.980000   \n",
       "39           0.980392           0.960784           0.979167         0.973333   \n",
       "40           0.980392           0.980392           1.000000         0.986667   \n",
       "41           0.980392           0.960784           0.979167         0.973333   \n",
       "42           0.980392           0.960784           0.979167         0.973333   \n",
       "43           0.980392           0.980392           0.979167         0.980000   \n",
       "44           0.960784           0.980392           0.979167         0.973333   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0         0.033333               36            1.000000            1.000000   \n",
       "1         0.008782               41            0.969697            0.989899   \n",
       "2         0.015925                6            0.949495            0.989899   \n",
       "3         0.009021               16            0.949495            0.979798   \n",
       "4         0.009147                1            0.959596            0.979798   \n",
       "5         0.009021               16            0.959596            0.979798   \n",
       "6         0.009021               16            0.959596            0.989899   \n",
       "7         0.000572                6            0.969697            0.979798   \n",
       "8         0.009021               16            0.959596            0.979798   \n",
       "9         0.033333               36            1.000000            1.000000   \n",
       "10        0.008782               41            0.969697            0.989899   \n",
       "11        0.015925                6            0.949495            0.989899   \n",
       "12        0.009021               16            0.949495            0.979798   \n",
       "13        0.009147                1            0.959596            0.979798   \n",
       "14        0.009021               16            0.959596            0.979798   \n",
       "15        0.009021               16            0.959596            0.989899   \n",
       "16        0.000572                6            0.969697            0.979798   \n",
       "17        0.009021               16            0.959596            0.979798   \n",
       "18        0.033333               36            1.000000            1.000000   \n",
       "19        0.008782               41            0.969697            0.989899   \n",
       "20        0.015925                6            0.949495            0.989899   \n",
       "21        0.009021               16            0.949495            0.979798   \n",
       "22        0.009147                1            0.959596            0.979798   \n",
       "23        0.009021               16            0.959596            0.979798   \n",
       "24        0.009021               16            0.959596            0.989899   \n",
       "25        0.000572                6            0.969697            0.979798   \n",
       "26        0.009021               16            0.959596            0.979798   \n",
       "27        0.033333               36            1.000000            1.000000   \n",
       "28        0.008782               41            0.969697            0.989899   \n",
       "29        0.015925                6            0.949495            0.989899   \n",
       "30        0.009021               16            0.949495            0.979798   \n",
       "31        0.009147                1            0.959596            0.979798   \n",
       "32        0.009021               16            0.959596            0.979798   \n",
       "33        0.009021               16            0.959596            0.989899   \n",
       "34        0.000572                6            0.969697            0.979798   \n",
       "35        0.009021               16            0.959596            0.979798   \n",
       "36        0.033333               36            1.000000            1.000000   \n",
       "37        0.008782               41            0.969697            0.989899   \n",
       "38        0.015925                6            0.949495            0.989899   \n",
       "39        0.009021               16            0.949495            0.979798   \n",
       "40        0.009147                1            0.959596            0.979798   \n",
       "41        0.009021               16            0.959596            0.979798   \n",
       "42        0.009021               16            0.959596            0.989899   \n",
       "43        0.000572                6            0.969697            0.979798   \n",
       "44        0.009021               16            0.959596            0.979798   \n",
       "\n",
       "    split2_train_score  mean_train_score  std_train_score  \n",
       "0             1.000000          1.000000         0.000000  \n",
       "1             0.970588          0.976728         0.009320  \n",
       "2             0.950980          0.963458         0.018706  \n",
       "3             0.960784          0.963359         0.012504  \n",
       "4             0.960784          0.966726         0.009256  \n",
       "5             0.960784          0.966726         0.009256  \n",
       "6             0.970588          0.973361         0.012526  \n",
       "7             0.960784          0.970093         0.007767  \n",
       "8             0.960784          0.966726         0.009256  \n",
       "9             1.000000          1.000000         0.000000  \n",
       "10            0.970588          0.976728         0.009320  \n",
       "11            0.950980          0.963458         0.018706  \n",
       "12            0.960784          0.963359         0.012504  \n",
       "13            0.960784          0.966726         0.009256  \n",
       "14            0.960784          0.966726         0.009256  \n",
       "15            0.970588          0.973361         0.012526  \n",
       "16            0.960784          0.970093         0.007767  \n",
       "17            0.960784          0.966726         0.009256  \n",
       "18            1.000000          1.000000         0.000000  \n",
       "19            0.970588          0.976728         0.009320  \n",
       "20            0.950980          0.963458         0.018706  \n",
       "21            0.960784          0.963359         0.012504  \n",
       "22            0.960784          0.966726         0.009256  \n",
       "23            0.960784          0.966726         0.009256  \n",
       "24            0.970588          0.973361         0.012526  \n",
       "25            0.960784          0.970093         0.007767  \n",
       "26            0.960784          0.966726         0.009256  \n",
       "27            1.000000          1.000000         0.000000  \n",
       "28            0.970588          0.976728         0.009320  \n",
       "29            0.950980          0.963458         0.018706  \n",
       "30            0.960784          0.963359         0.012504  \n",
       "31            0.960784          0.966726         0.009256  \n",
       "32            0.960784          0.966726         0.009256  \n",
       "33            0.970588          0.973361         0.012526  \n",
       "34            0.960784          0.970093         0.007767  \n",
       "35            0.960784          0.966726         0.009256  \n",
       "36            1.000000          1.000000         0.000000  \n",
       "37            0.970588          0.976728         0.009320  \n",
       "38            0.950980          0.963458         0.018706  \n",
       "39            0.960784          0.963359         0.012504  \n",
       "40            0.960784          0.966726         0.009256  \n",
       "41            0.960784          0.966726         0.009256  \n",
       "42            0.970588          0.973361         0.012526  \n",
       "43            0.960784          0.970093         0.007767  \n",
       "44            0.960784          0.966726         0.009256  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>0.00233181</td>\n",
       "      <td>0.00149973</td>\n",
       "      <td>0.00183336</td>\n",
       "      <td>0.00166694</td>\n",
       "      <td>0.00200518</td>\n",
       "      <td>0.00183352</td>\n",
       "      <td>0.00183304</td>\n",
       "      <td>0.00166702</td>\n",
       "      <td>0.00216277</td>\n",
       "      <td>0.00216985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.00116666</td>\n",
       "      <td>0.00149949</td>\n",
       "      <td>0.00166726</td>\n",
       "      <td>0.0014991</td>\n",
       "      <td>0.00150013</td>\n",
       "      <td>0.00166059</td>\n",
       "      <td>0.00233301</td>\n",
       "      <td>0.00166591</td>\n",
       "      <td>0.00199954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.000234393</td>\n",
       "      <td>1.12392e-07</td>\n",
       "      <td>0.000235966</td>\n",
       "      <td>0.000236079</td>\n",
       "      <td>0.000408666</td>\n",
       "      <td>0.000236079</td>\n",
       "      <td>0.000473846</td>\n",
       "      <td>0.00023636</td>\n",
       "      <td>0.00023703</td>\n",
       "      <td>0.000238049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000236416</td>\n",
       "      <td>0.00023546</td>\n",
       "      <td>1.10693e-06</td>\n",
       "      <td>0.000235348</td>\n",
       "      <td>1.65563e-06</td>\n",
       "      <td>1.94668e-07</td>\n",
       "      <td>0.000248618</td>\n",
       "      <td>0.000235573</td>\n",
       "      <td>0.00023681</td>\n",
       "      <td>0.000408511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.0030009</td>\n",
       "      <td>0.00249879</td>\n",
       "      <td>0.00233205</td>\n",
       "      <td>0.00249894</td>\n",
       "      <td>0.00266051</td>\n",
       "      <td>0.00233173</td>\n",
       "      <td>0.00266512</td>\n",
       "      <td>0.00366696</td>\n",
       "      <td>0.0026652</td>\n",
       "      <td>0.00367673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00283313</td>\n",
       "      <td>0.00233221</td>\n",
       "      <td>0.00233221</td>\n",
       "      <td>0.00216548</td>\n",
       "      <td>0.00216619</td>\n",
       "      <td>0.00216579</td>\n",
       "      <td>0.00316691</td>\n",
       "      <td>0.00316628</td>\n",
       "      <td>0.00283329</td>\n",
       "      <td>0.00283249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.000410166</td>\n",
       "      <td>2.9736e-07</td>\n",
       "      <td>0.000235911</td>\n",
       "      <td>4.05234e-07</td>\n",
       "      <td>0.000239646</td>\n",
       "      <td>0.000235517</td>\n",
       "      <td>0.000236137</td>\n",
       "      <td>0.00102705</td>\n",
       "      <td>0.000236588</td>\n",
       "      <td>0.000855205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000472719</td>\n",
       "      <td>0.000236191</td>\n",
       "      <td>0.000235854</td>\n",
       "      <td>0.000235741</td>\n",
       "      <td>0.000236079</td>\n",
       "      <td>0.00023636</td>\n",
       "      <td>0.000623756</td>\n",
       "      <td>0.000236809</td>\n",
       "      <td>0.000236978</td>\n",
       "      <td>0.000235236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_leaf_size</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_n_neighbors</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 1}</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 2}</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 3}</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 4}</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 5}</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 6}</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 7}</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 8}</td>\n",
       "      <td>{'leaf_size': 10, 'n_neighbors': 9}</td>\n",
       "      <td>{'leaf_size': 20, 'n_neighbors': 1}</td>\n",
       "      <td>...</td>\n",
       "      <td>{'leaf_size': 40, 'n_neighbors': 9}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 1}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 2}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 3}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 4}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 5}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 6}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 7}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 8}</td>\n",
       "      <td>{'leaf_size': 50, 'n_neighbors': 9}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_score</th>\n",
       "      <td>1</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>1</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.979167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.973333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.0333333</td>\n",
       "      <td>0.00878204</td>\n",
       "      <td>0.0159247</td>\n",
       "      <td>0.00902067</td>\n",
       "      <td>0.00914659</td>\n",
       "      <td>0.00902067</td>\n",
       "      <td>0.00902067</td>\n",
       "      <td>0.000571662</td>\n",
       "      <td>0.00902067</td>\n",
       "      <td>0.0333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00902067</td>\n",
       "      <td>0.0333333</td>\n",
       "      <td>0.00878204</td>\n",
       "      <td>0.0159247</td>\n",
       "      <td>0.00902067</td>\n",
       "      <td>0.00914659</td>\n",
       "      <td>0.00902067</td>\n",
       "      <td>0.00902067</td>\n",
       "      <td>0.000571662</td>\n",
       "      <td>0.00902067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <td>36</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>36</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_train_score</th>\n",
       "      <td>1</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>1</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.959596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_train_score</th>\n",
       "      <td>1</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>1</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.979798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_train_score</th>\n",
       "      <td>1</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.95098</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.95098</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_score</th>\n",
       "      <td>1</td>\n",
       "      <td>0.976728</td>\n",
       "      <td>0.963458</td>\n",
       "      <td>0.963359</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.973361</td>\n",
       "      <td>0.970093</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>1</td>\n",
       "      <td>0.976728</td>\n",
       "      <td>0.963458</td>\n",
       "      <td>0.963359</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>0.973361</td>\n",
       "      <td>0.970093</td>\n",
       "      <td>0.966726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_train_score</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00932036</td>\n",
       "      <td>0.0187064</td>\n",
       "      <td>0.0125044</td>\n",
       "      <td>0.00925595</td>\n",
       "      <td>0.00925595</td>\n",
       "      <td>0.0125256</td>\n",
       "      <td>0.00776735</td>\n",
       "      <td>0.00925595</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00925595</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00932036</td>\n",
       "      <td>0.0187064</td>\n",
       "      <td>0.0125044</td>\n",
       "      <td>0.00925595</td>\n",
       "      <td>0.00925595</td>\n",
       "      <td>0.0125256</td>\n",
       "      <td>0.00776735</td>\n",
       "      <td>0.00925595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0   \\\n",
       "mean_fit_time                                0.00233181   \n",
       "std_fit_time                                0.000234393   \n",
       "mean_score_time                               0.0030009   \n",
       "std_score_time                              0.000410166   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     1   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 1}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.921569   \n",
       "split2_test_score                                     1   \n",
       "mean_test_score                                0.966667   \n",
       "std_test_score                                0.0333333   \n",
       "rank_test_score                                      36   \n",
       "split0_train_score                                    1   \n",
       "split1_train_score                                    1   \n",
       "split2_train_score                                    1   \n",
       "mean_train_score                                      1   \n",
       "std_train_score                                       0   \n",
       "\n",
       "                                                     1   \\\n",
       "mean_fit_time                                0.00149973   \n",
       "std_fit_time                                1.12392e-07   \n",
       "mean_score_time                              0.00249879   \n",
       "std_score_time                               2.9736e-07   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     2   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 2}   \n",
       "split0_test_score                              0.960784   \n",
       "split1_test_score                              0.941176   \n",
       "split2_test_score                              0.958333   \n",
       "mean_test_score                                0.953333   \n",
       "std_test_score                               0.00878204   \n",
       "rank_test_score                                      41   \n",
       "split0_train_score                             0.969697   \n",
       "split1_train_score                             0.989899   \n",
       "split2_train_score                             0.970588   \n",
       "mean_train_score                               0.976728   \n",
       "std_train_score                              0.00932036   \n",
       "\n",
       "                                                     2   \\\n",
       "mean_fit_time                                0.00183336   \n",
       "std_fit_time                                0.000235966   \n",
       "mean_score_time                              0.00233205   \n",
       "std_score_time                              0.000235911   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     3   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 3}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.960784   \n",
       "split2_test_score                                     1   \n",
       "mean_test_score                                    0.98   \n",
       "std_test_score                                0.0159247   \n",
       "rank_test_score                                       6   \n",
       "split0_train_score                             0.949495   \n",
       "split1_train_score                             0.989899   \n",
       "split2_train_score                              0.95098   \n",
       "mean_train_score                               0.963458   \n",
       "std_train_score                               0.0187064   \n",
       "\n",
       "                                                     3   \\\n",
       "mean_fit_time                                0.00166694   \n",
       "std_fit_time                                0.000236079   \n",
       "mean_score_time                              0.00249894   \n",
       "std_score_time                              4.05234e-07   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     4   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 4}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.960784   \n",
       "split2_test_score                              0.979167   \n",
       "mean_test_score                                0.973333   \n",
       "std_test_score                               0.00902067   \n",
       "rank_test_score                                      16   \n",
       "split0_train_score                             0.949495   \n",
       "split1_train_score                             0.979798   \n",
       "split2_train_score                             0.960784   \n",
       "mean_train_score                               0.963359   \n",
       "std_train_score                               0.0125044   \n",
       "\n",
       "                                                     4   \\\n",
       "mean_fit_time                                0.00200518   \n",
       "std_fit_time                                0.000408666   \n",
       "mean_score_time                              0.00266051   \n",
       "std_score_time                              0.000239646   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     5   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 5}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.980392   \n",
       "split2_test_score                                     1   \n",
       "mean_test_score                                0.986667   \n",
       "std_test_score                               0.00914659   \n",
       "rank_test_score                                       1   \n",
       "split0_train_score                             0.959596   \n",
       "split1_train_score                             0.979798   \n",
       "split2_train_score                             0.960784   \n",
       "mean_train_score                               0.966726   \n",
       "std_train_score                              0.00925595   \n",
       "\n",
       "                                                     5   \\\n",
       "mean_fit_time                                0.00183352   \n",
       "std_fit_time                                0.000236079   \n",
       "mean_score_time                              0.00233173   \n",
       "std_score_time                              0.000235517   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     6   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 6}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.960784   \n",
       "split2_test_score                              0.979167   \n",
       "mean_test_score                                0.973333   \n",
       "std_test_score                               0.00902067   \n",
       "rank_test_score                                      16   \n",
       "split0_train_score                             0.959596   \n",
       "split1_train_score                             0.979798   \n",
       "split2_train_score                             0.960784   \n",
       "mean_train_score                               0.966726   \n",
       "std_train_score                              0.00925595   \n",
       "\n",
       "                                                     6   \\\n",
       "mean_fit_time                                0.00183304   \n",
       "std_fit_time                                0.000473846   \n",
       "mean_score_time                              0.00266512   \n",
       "std_score_time                              0.000236137   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     7   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 7}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.960784   \n",
       "split2_test_score                              0.979167   \n",
       "mean_test_score                                0.973333   \n",
       "std_test_score                               0.00902067   \n",
       "rank_test_score                                      16   \n",
       "split0_train_score                             0.959596   \n",
       "split1_train_score                             0.989899   \n",
       "split2_train_score                             0.970588   \n",
       "mean_train_score                               0.973361   \n",
       "std_train_score                               0.0125256   \n",
       "\n",
       "                                                     7   \\\n",
       "mean_fit_time                                0.00166702   \n",
       "std_fit_time                                 0.00023636   \n",
       "mean_score_time                              0.00366696   \n",
       "std_score_time                               0.00102705   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     8   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 8}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.980392   \n",
       "split2_test_score                              0.979167   \n",
       "mean_test_score                                    0.98   \n",
       "std_test_score                              0.000571662   \n",
       "rank_test_score                                       6   \n",
       "split0_train_score                             0.969697   \n",
       "split1_train_score                             0.979798   \n",
       "split2_train_score                             0.960784   \n",
       "mean_train_score                               0.970093   \n",
       "std_train_score                              0.00776735   \n",
       "\n",
       "                                                     8   \\\n",
       "mean_fit_time                                0.00216277   \n",
       "std_fit_time                                 0.00023703   \n",
       "mean_score_time                               0.0026652   \n",
       "std_score_time                              0.000236588   \n",
       "param_leaf_size                                      10   \n",
       "param_n_neighbors                                     9   \n",
       "params              {'leaf_size': 10, 'n_neighbors': 9}   \n",
       "split0_test_score                              0.960784   \n",
       "split1_test_score                              0.980392   \n",
       "split2_test_score                              0.979167   \n",
       "mean_test_score                                0.973333   \n",
       "std_test_score                               0.00902067   \n",
       "rank_test_score                                      16   \n",
       "split0_train_score                             0.959596   \n",
       "split1_train_score                             0.979798   \n",
       "split2_train_score                             0.960784   \n",
       "mean_train_score                               0.966726   \n",
       "std_train_score                              0.00925595   \n",
       "\n",
       "                                                     9   ...  \\\n",
       "mean_fit_time                                0.00216985  ...   \n",
       "std_fit_time                                0.000238049  ...   \n",
       "mean_score_time                              0.00367673  ...   \n",
       "std_score_time                              0.000855205  ...   \n",
       "param_leaf_size                                      20  ...   \n",
       "param_n_neighbors                                     1  ...   \n",
       "params              {'leaf_size': 20, 'n_neighbors': 1}  ...   \n",
       "split0_test_score                              0.980392  ...   \n",
       "split1_test_score                              0.921569  ...   \n",
       "split2_test_score                                     1  ...   \n",
       "mean_test_score                                0.966667  ...   \n",
       "std_test_score                                0.0333333  ...   \n",
       "rank_test_score                                      36  ...   \n",
       "split0_train_score                                    1  ...   \n",
       "split1_train_score                                    1  ...   \n",
       "split2_train_score                                    1  ...   \n",
       "mean_train_score                                      1  ...   \n",
       "std_train_score                                       0  ...   \n",
       "\n",
       "                                                     35  \\\n",
       "mean_fit_time                                  0.001834   \n",
       "std_fit_time                                0.000236416   \n",
       "mean_score_time                              0.00283313   \n",
       "std_score_time                              0.000472719   \n",
       "param_leaf_size                                      40   \n",
       "param_n_neighbors                                     9   \n",
       "params              {'leaf_size': 40, 'n_neighbors': 9}   \n",
       "split0_test_score                              0.960784   \n",
       "split1_test_score                              0.980392   \n",
       "split2_test_score                              0.979167   \n",
       "mean_test_score                                0.973333   \n",
       "std_test_score                               0.00902067   \n",
       "rank_test_score                                      16   \n",
       "split0_train_score                             0.959596   \n",
       "split1_train_score                             0.979798   \n",
       "split2_train_score                             0.960784   \n",
       "mean_train_score                               0.966726   \n",
       "std_train_score                              0.00925595   \n",
       "\n",
       "                                                     36  \\\n",
       "mean_fit_time                                0.00116666   \n",
       "std_fit_time                                 0.00023546   \n",
       "mean_score_time                              0.00233221   \n",
       "std_score_time                              0.000236191   \n",
       "param_leaf_size                                      50   \n",
       "param_n_neighbors                                     1   \n",
       "params              {'leaf_size': 50, 'n_neighbors': 1}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.921569   \n",
       "split2_test_score                                     1   \n",
       "mean_test_score                                0.966667   \n",
       "std_test_score                                0.0333333   \n",
       "rank_test_score                                      36   \n",
       "split0_train_score                                    1   \n",
       "split1_train_score                                    1   \n",
       "split2_train_score                                    1   \n",
       "mean_train_score                                      1   \n",
       "std_train_score                                       0   \n",
       "\n",
       "                                                     37  \\\n",
       "mean_fit_time                                0.00149949   \n",
       "std_fit_time                                1.10693e-06   \n",
       "mean_score_time                              0.00233221   \n",
       "std_score_time                              0.000235854   \n",
       "param_leaf_size                                      50   \n",
       "param_n_neighbors                                     2   \n",
       "params              {'leaf_size': 50, 'n_neighbors': 2}   \n",
       "split0_test_score                              0.960784   \n",
       "split1_test_score                              0.941176   \n",
       "split2_test_score                              0.958333   \n",
       "mean_test_score                                0.953333   \n",
       "std_test_score                               0.00878204   \n",
       "rank_test_score                                      41   \n",
       "split0_train_score                             0.969697   \n",
       "split1_train_score                             0.989899   \n",
       "split2_train_score                             0.970588   \n",
       "mean_train_score                               0.976728   \n",
       "std_train_score                              0.00932036   \n",
       "\n",
       "                                                     38  \\\n",
       "mean_fit_time                                0.00166726   \n",
       "std_fit_time                                0.000235348   \n",
       "mean_score_time                              0.00216548   \n",
       "std_score_time                              0.000235741   \n",
       "param_leaf_size                                      50   \n",
       "param_n_neighbors                                     3   \n",
       "params              {'leaf_size': 50, 'n_neighbors': 3}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.960784   \n",
       "split2_test_score                                     1   \n",
       "mean_test_score                                    0.98   \n",
       "std_test_score                                0.0159247   \n",
       "rank_test_score                                       6   \n",
       "split0_train_score                             0.949495   \n",
       "split1_train_score                             0.989899   \n",
       "split2_train_score                              0.95098   \n",
       "mean_train_score                               0.963458   \n",
       "std_train_score                               0.0187064   \n",
       "\n",
       "                                                     39  \\\n",
       "mean_fit_time                                 0.0014991   \n",
       "std_fit_time                                1.65563e-06   \n",
       "mean_score_time                              0.00216619   \n",
       "std_score_time                              0.000236079   \n",
       "param_leaf_size                                      50   \n",
       "param_n_neighbors                                     4   \n",
       "params              {'leaf_size': 50, 'n_neighbors': 4}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.960784   \n",
       "split2_test_score                              0.979167   \n",
       "mean_test_score                                0.973333   \n",
       "std_test_score                               0.00902067   \n",
       "rank_test_score                                      16   \n",
       "split0_train_score                             0.949495   \n",
       "split1_train_score                             0.979798   \n",
       "split2_train_score                             0.960784   \n",
       "mean_train_score                               0.963359   \n",
       "std_train_score                               0.0125044   \n",
       "\n",
       "                                                     40  \\\n",
       "mean_fit_time                                0.00150013   \n",
       "std_fit_time                                1.94668e-07   \n",
       "mean_score_time                              0.00216579   \n",
       "std_score_time                               0.00023636   \n",
       "param_leaf_size                                      50   \n",
       "param_n_neighbors                                     5   \n",
       "params              {'leaf_size': 50, 'n_neighbors': 5}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.980392   \n",
       "split2_test_score                                     1   \n",
       "mean_test_score                                0.986667   \n",
       "std_test_score                               0.00914659   \n",
       "rank_test_score                                       1   \n",
       "split0_train_score                             0.959596   \n",
       "split1_train_score                             0.979798   \n",
       "split2_train_score                             0.960784   \n",
       "mean_train_score                               0.966726   \n",
       "std_train_score                              0.00925595   \n",
       "\n",
       "                                                     41  \\\n",
       "mean_fit_time                                0.00166059   \n",
       "std_fit_time                                0.000248618   \n",
       "mean_score_time                              0.00316691   \n",
       "std_score_time                              0.000623756   \n",
       "param_leaf_size                                      50   \n",
       "param_n_neighbors                                     6   \n",
       "params              {'leaf_size': 50, 'n_neighbors': 6}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.960784   \n",
       "split2_test_score                              0.979167   \n",
       "mean_test_score                                0.973333   \n",
       "std_test_score                               0.00902067   \n",
       "rank_test_score                                      16   \n",
       "split0_train_score                             0.959596   \n",
       "split1_train_score                             0.979798   \n",
       "split2_train_score                             0.960784   \n",
       "mean_train_score                               0.966726   \n",
       "std_train_score                              0.00925595   \n",
       "\n",
       "                                                     42  \\\n",
       "mean_fit_time                                0.00233301   \n",
       "std_fit_time                                0.000235573   \n",
       "mean_score_time                              0.00316628   \n",
       "std_score_time                              0.000236809   \n",
       "param_leaf_size                                      50   \n",
       "param_n_neighbors                                     7   \n",
       "params              {'leaf_size': 50, 'n_neighbors': 7}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.960784   \n",
       "split2_test_score                              0.979167   \n",
       "mean_test_score                                0.973333   \n",
       "std_test_score                               0.00902067   \n",
       "rank_test_score                                      16   \n",
       "split0_train_score                             0.959596   \n",
       "split1_train_score                             0.989899   \n",
       "split2_train_score                             0.970588   \n",
       "mean_train_score                               0.973361   \n",
       "std_train_score                               0.0125256   \n",
       "\n",
       "                                                     43  \\\n",
       "mean_fit_time                                0.00166591   \n",
       "std_fit_time                                 0.00023681   \n",
       "mean_score_time                              0.00283329   \n",
       "std_score_time                              0.000236978   \n",
       "param_leaf_size                                      50   \n",
       "param_n_neighbors                                     8   \n",
       "params              {'leaf_size': 50, 'n_neighbors': 8}   \n",
       "split0_test_score                              0.980392   \n",
       "split1_test_score                              0.980392   \n",
       "split2_test_score                              0.979167   \n",
       "mean_test_score                                    0.98   \n",
       "std_test_score                              0.000571662   \n",
       "rank_test_score                                       6   \n",
       "split0_train_score                             0.969697   \n",
       "split1_train_score                             0.979798   \n",
       "split2_train_score                             0.960784   \n",
       "mean_train_score                               0.970093   \n",
       "std_train_score                              0.00776735   \n",
       "\n",
       "                                                     44  \n",
       "mean_fit_time                                0.00199954  \n",
       "std_fit_time                                0.000408511  \n",
       "mean_score_time                              0.00283249  \n",
       "std_score_time                              0.000235236  \n",
       "param_leaf_size                                      50  \n",
       "param_n_neighbors                                     9  \n",
       "params              {'leaf_size': 50, 'n_neighbors': 9}  \n",
       "split0_test_score                              0.960784  \n",
       "split1_test_score                              0.980392  \n",
       "split2_test_score                              0.979167  \n",
       "mean_test_score                                0.973333  \n",
       "std_test_score                               0.00902067  \n",
       "rank_test_score                                      16  \n",
       "split0_train_score                             0.959596  \n",
       "split1_train_score                             0.979798  \n",
       "split2_train_score                             0.960784  \n",
       "mean_train_score                               0.966726  \n",
       "std_train_score                              0.00925595  \n",
       "\n",
       "[18 rows x 45 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width    species\n",
       "0             5.1          3.5           1.4          0.2     setosa\n",
       "1             4.9          3.0           1.4          0.2     setosa\n",
       "2             4.7          3.2           1.3          0.2     setosa\n",
       "3             4.6          3.1           1.5          0.2     setosa\n",
       "4             5.0          3.6           1.4          0.2     setosa\n",
       "5             5.4          3.9           1.7          0.4     setosa\n",
       "6             4.6          3.4           1.4          0.3     setosa\n",
       "7             5.0          3.4           1.5          0.2     setosa\n",
       "8             4.4          2.9           1.4          0.2     setosa\n",
       "9             4.9          3.1           1.5          0.1     setosa\n",
       "10            5.4          3.7           1.5          0.2     setosa\n",
       "11            4.8          3.4           1.6          0.2     setosa\n",
       "12            4.8          3.0           1.4          0.1     setosa\n",
       "13            4.3          3.0           1.1          0.1     setosa\n",
       "14            5.8          4.0           1.2          0.2     setosa\n",
       "15            5.7          4.4           1.5          0.4     setosa\n",
       "16            5.4          3.9           1.3          0.4     setosa\n",
       "17            5.1          3.5           1.4          0.3     setosa\n",
       "18            5.7          3.8           1.7          0.3     setosa\n",
       "19            5.1          3.8           1.5          0.3     setosa\n",
       "20            5.4          3.4           1.7          0.2     setosa\n",
       "21            5.1          3.7           1.5          0.4     setosa\n",
       "22            4.6          3.6           1.0          0.2     setosa\n",
       "23            5.1          3.3           1.7          0.5     setosa\n",
       "24            4.8          3.4           1.9          0.2     setosa\n",
       "25            5.0          3.0           1.6          0.2     setosa\n",
       "26            5.0          3.4           1.6          0.4     setosa\n",
       "27            5.2          3.5           1.5          0.2     setosa\n",
       "28            5.2          3.4           1.4          0.2     setosa\n",
       "29            4.7          3.2           1.6          0.2     setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "120           6.9          3.2           5.7          2.3  virginica\n",
       "121           5.6          2.8           4.9          2.0  virginica\n",
       "122           7.7          2.8           6.7          2.0  virginica\n",
       "123           6.3          2.7           4.9          1.8  virginica\n",
       "124           6.7          3.3           5.7          2.1  virginica\n",
       "125           7.2          3.2           6.0          1.8  virginica\n",
       "126           6.2          2.8           4.8          1.8  virginica\n",
       "127           6.1          3.0           4.9          1.8  virginica\n",
       "128           6.4          2.8           5.6          2.1  virginica\n",
       "129           7.2          3.0           5.8          1.6  virginica\n",
       "130           7.4          2.8           6.1          1.9  virginica\n",
       "131           7.9          3.8           6.4          2.0  virginica\n",
       "132           6.4          2.8           5.6          2.2  virginica\n",
       "133           6.3          2.8           5.1          1.5  virginica\n",
       "134           6.1          2.6           5.6          1.4  virginica\n",
       "135           7.7          3.0           6.1          2.3  virginica\n",
       "136           6.3          3.4           5.6          2.4  virginica\n",
       "137           6.4          3.1           5.5          1.8  virginica\n",
       "138           6.0          3.0           4.8          1.8  virginica\n",
       "139           6.9          3.1           5.4          2.1  virginica\n",
       "140           6.7          3.1           5.6          2.4  virginica\n",
       "141           6.9          3.1           5.1          2.3  virginica\n",
       "142           5.8          2.7           5.1          1.9  virginica\n",
       "143           6.8          3.2           5.9          2.3  virginica\n",
       "144           6.7          3.3           5.7          2.5  virginica\n",
       "145           6.7          3.0           5.2          2.3  virginica\n",
       "146           6.3          2.5           5.0          1.9  virginica\n",
       "147           6.5          3.0           5.2          2.0  virginica\n",
       "148           6.2          3.4           5.4          2.3  virginica\n",
       "149           5.9          3.0           5.1          1.8  virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> 문자를 숫자로 바꿔야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문자를 숫자로 바꾸는 것 : Label encoding\n",
    "## scikit은 target 일때, 자동적으로 label encoding해주는 함수들이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "5      0\n",
       "6      0\n",
       "7      0\n",
       "8      0\n",
       "9      0\n",
       "10     0\n",
       "11     0\n",
       "12     0\n",
       "13     0\n",
       "14     0\n",
       "15     0\n",
       "16     0\n",
       "17     0\n",
       "18     0\n",
       "19     0\n",
       "20     0\n",
       "21     0\n",
       "22     0\n",
       "23     0\n",
       "24     0\n",
       "25     0\n",
       "26     0\n",
       "27     0\n",
       "28     0\n",
       "29     0\n",
       "      ..\n",
       "120    1\n",
       "121    1\n",
       "122    1\n",
       "123    1\n",
       "124    1\n",
       "125    1\n",
       "126    1\n",
       "127    1\n",
       "128    1\n",
       "129    1\n",
       "130    1\n",
       "131    1\n",
       "132    1\n",
       "133    1\n",
       "134    1\n",
       "135    1\n",
       "136    1\n",
       "137    1\n",
       "138    1\n",
       "139    1\n",
       "140    1\n",
       "141    1\n",
       "142    1\n",
       "143    1\n",
       "144    1\n",
       "145    1\n",
       "146    1\n",
       "147    1\n",
       "148    1\n",
       "149    1\n",
       "Name: species, Length: 150, dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.species.map({'setosa':0,'virginica':1,'versicolor':2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라벨 인코딩을 x에 하면 바보짓이 된다, 숫자의 크기가 영향을 미칠 수 있다.\n",
    "\n",
    "카테고리컬 데이터는 라벨링을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(iris.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.transform(iris.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit_transform(iris.species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit Labeling 쉽다잉.\n",
    "\n",
    "인베딩 방식 문자를 한꺼번에 숫자로 변경한다.\n",
    "\n",
    "## 2. OneHotEncoder ( 더미 변수 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     setosa  versicolor  virginica\n",
       "0         1           0          0\n",
       "1         1           0          0\n",
       "2         1           0          0\n",
       "3         1           0          0\n",
       "4         1           0          0\n",
       "5         1           0          0\n",
       "6         1           0          0\n",
       "7         1           0          0\n",
       "8         1           0          0\n",
       "9         1           0          0\n",
       "10        1           0          0\n",
       "11        1           0          0\n",
       "12        1           0          0\n",
       "13        1           0          0\n",
       "14        1           0          0\n",
       "15        1           0          0\n",
       "16        1           0          0\n",
       "17        1           0          0\n",
       "18        1           0          0\n",
       "19        1           0          0\n",
       "20        1           0          0\n",
       "21        1           0          0\n",
       "22        1           0          0\n",
       "23        1           0          0\n",
       "24        1           0          0\n",
       "25        1           0          0\n",
       "26        1           0          0\n",
       "27        1           0          0\n",
       "28        1           0          0\n",
       "29        1           0          0\n",
       "..      ...         ...        ...\n",
       "120       0           0          1\n",
       "121       0           0          1\n",
       "122       0           0          1\n",
       "123       0           0          1\n",
       "124       0           0          1\n",
       "125       0           0          1\n",
       "126       0           0          1\n",
       "127       0           0          1\n",
       "128       0           0          1\n",
       "129       0           0          1\n",
       "130       0           0          1\n",
       "131       0           0          1\n",
       "132       0           0          1\n",
       "133       0           0          1\n",
       "134       0           0          1\n",
       "135       0           0          1\n",
       "136       0           0          1\n",
       "137       0           0          1\n",
       "138       0           0          1\n",
       "139       0           0          1\n",
       "140       0           0          1\n",
       "141       0           0          1\n",
       "142       0           0          1\n",
       "143       0           0          1\n",
       "144       0           0          1\n",
       "145       0           0          1\n",
       "146       0           0          1\n",
       "147       0           0          1\n",
       "148       0           0          1\n",
       "149       0           0          1\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(iris.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=['setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n 'setosa' 'setosa' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n 'virginica' 'virginica' 'virginica'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-155-811c3c513631>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mohe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, handle_unknown)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \"\"\"\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mX_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dtype'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    550\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=['setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n 'setosa' 'setosa' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n 'versicolor' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n 'virginica' 'virginica' 'virginica'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "ohe.fit(iris.species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D필요하다 팬씨 인덱싱으로 처리해주야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<150x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 150 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit_transform(iris[['species']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.fit_transform(iris[['species']]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit은 변환시킨것을 거꿀로 변환시켜준다.\n",
    "\n",
    "pandas는 엄따"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['setosa']], dtype=object)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.inverse_transform([[1.,0.,0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['virginica'], dtype=object)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform([2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But. scikit은 차원이 증가하면서 차원의 저주에 빠질 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Deep Learning 기반은 Normalize해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler,RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scairing -> RobustScaler\n",
    "\n",
    "meannmax scaler + standard 많이 한다.\n",
    "\n",
    "robustScaler : outlier가 있을 때.\n",
    "\n",
    "타겟값은 scaling을 안하는게 좋다.\n",
    "\n",
    "regration할 때 scaling을 많이 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c9b7a28e80>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "값의 상대적인 크기로 판별할 때 scaling을 하지 않으면 전체적인 평균이 떨어진다.\n",
    "\n",
    "정말 중요한 값(각각의 성질이 중요할때)일 때는 scaling하면 안된다. -> 도메인 지식이 필요로 한다.\n",
    "\n",
    "-> 중요하다하면 encoding을 다르게 한다.(단편적으로 바꾸면 안된다)\n",
    "\n",
    "-> weight를 준다.\n",
    "\n",
    "empirical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      6.1\n",
       "1      5.9\n",
       "2      5.7\n",
       "3      5.6\n",
       "4      6.0\n",
       "5      6.4\n",
       "6      5.6\n",
       "7      6.0\n",
       "8      5.4\n",
       "9      5.9\n",
       "10     6.4\n",
       "11     5.8\n",
       "12     5.8\n",
       "13     5.3\n",
       "14     6.8\n",
       "15     6.7\n",
       "16     6.4\n",
       "17     6.1\n",
       "18     6.7\n",
       "19     6.1\n",
       "20     6.4\n",
       "21     6.1\n",
       "22     5.6\n",
       "23     6.1\n",
       "24     5.8\n",
       "25     6.0\n",
       "26     6.0\n",
       "27     6.2\n",
       "28     6.2\n",
       "29     5.7\n",
       "      ... \n",
       "120    7.9\n",
       "121    6.6\n",
       "122    8.7\n",
       "123    7.3\n",
       "124    7.7\n",
       "125    8.2\n",
       "126    7.2\n",
       "127    7.1\n",
       "128    7.4\n",
       "129    8.2\n",
       "130    8.4\n",
       "131    8.9\n",
       "132    7.4\n",
       "133    7.3\n",
       "134    7.1\n",
       "135    8.7\n",
       "136    7.3\n",
       "137    7.4\n",
       "138    7.0\n",
       "139    7.9\n",
       "140    7.7\n",
       "141    7.9\n",
       "142    6.8\n",
       "143    7.8\n",
       "144    7.7\n",
       "145    7.7\n",
       "146    7.3\n",
       "147    7.5\n",
       "148    7.2\n",
       "149    6.9\n",
       "Name: sepal_length, Length: 150, dtype: float64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.sepal_length.map(lambda x:x+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map / apply를 이용해서 가중치를 조작할 수 있다.\n",
    "\n",
    "-> 예측할 때도 값을 변환시켜줘야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 잘됬는지 알수 있나?\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn-evaluation\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/25/1e4e6462aeffb185ada97c9ff5a7b7ca59544e418541e3babd59f7171c0b/sklearn-evaluation-0.5.tar.gz\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from sklearn-evaluation) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from sklearn-evaluation) (3.0.3)\n",
      "Requirement already satisfied: decorator in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from sklearn-evaluation) (4.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from sklearn-evaluation) (2.10)\n",
      "Collecting tabulate (from sklearn-evaluation)\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/fd/202954b3f0eb896c53b7b6f07390851b1fd2ca84aa95880d7ae4f434c4ac/tabulate-0.8.3.tar.gz (46kB)\n",
      "Requirement already satisfied: mistune in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from sklearn-evaluation) (0.8.4)\n",
      "Requirement already satisfied: numpy>=1.8.2 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn-evaluation) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.13.3 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn-evaluation) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from matplotlib->sklearn-evaluation) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from matplotlib->sklearn-evaluation) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from matplotlib->sklearn-evaluation) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from matplotlib->sklearn-evaluation) (2.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from jinja2->sklearn-evaluation) (1.1.1)\n",
      "Requirement already satisfied: six in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->sklearn-evaluation) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->sklearn-evaluation) (40.8.0)\n",
      "Building wheels for collected packages: sklearn-evaluation, tabulate\n",
      "  Building wheel for sklearn-evaluation (setup.py): started\n",
      "  Building wheel for sklearn-evaluation (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\scminnoG15\\AppData\\Local\\pip\\Cache\\wheels\\a8\\38\\0d\\9103d63a0189c6e3b4ecc6f03e40c1b6762b1ff55612a7313c\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\scminnoG15\\AppData\\Local\\pip\\Cache\\wheels\\2b\\67\\89\\414471314a2d15de625d184d8be6d38a03ae1e983dbda91e84\n",
      "Successfully built sklearn-evaluation tabulate\n",
      "Installing collected packages: tabulate, sklearn-evaluation\n",
      "Successfully installed sklearn-evaluation-0.5 tabulate-0.8.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ClassifierEvaluator',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'compute',\n",
       " 'evaluator',\n",
       " 'metrics',\n",
       " 'plot',\n",
       " 'preprocessing',\n",
       " 'report',\n",
       " 'util',\n",
       " 'validate']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sklearn_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'report'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking\n",
    "train_size,train_score, test_score = learning_curve(KNeighborsClassifier(),\n",
    "                                                   iris.iloc[:,:-1],iris.iloc[:,-1],\n",
    "                                                   cv = 10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13,  43,  74, 104, 135])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한눈에 안보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_evaluation.plot.learning_curve??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c9b8f31ef0>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sklearn_evaluation.plot.learning_curve(train_score,test_score,train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어느 정도 수렴하는 그래프를 그릴때 트레이닝을 체크한다.\n",
    "\n",
    "정체가 되어있다 -> 데이터가 모자라서x ->전처리가 안되서 or 잘못된데이터\n",
    "\n",
    "상향 그레프 -> 데이터를 더 필요로 한다.\n",
    "\n",
    "이 그래프로 데이터가 충분한지를 보여준다.\n",
    "\n",
    "딥러닝은 데이터를 많이 쓰기 때문에~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_curve??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gridsearchCV와 비슷하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score, test_score =validation_curve(KNeighborsClassifier(),iris.iloc[:,:-1],iris.iloc[:,-1],\n",
    "                'n_neighbors',range(3,10),\n",
    "                cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_evaluation.plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# 행-렬 : true-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This KNeighborsClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-188-b785741b9d4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\classification.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0m_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \"\"\"\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_fit_method\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_neighbors\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This KNeighborsClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "confusion_matrix(y_test,knn.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This KNeighborsClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-189-d57ffcaa5fa7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msklearn_evaluation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\classification.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0m_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \"\"\"\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_fit_method\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_neighbors\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This KNeighborsClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "sklearn_evaluation.plot.confusion_matrix(y_test,knn.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This KNeighborsClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-191-3be1f525200e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\classification.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0m_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \"\"\"\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_fit_method\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_neighbors\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This KNeighborsClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "classification_report(y_test,knn.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas-profiling\n",
      "  Downloading https://files.pythonhosted.org/packages/5d/17/3159aff8690f095ae5e21d779a405cca89234fa17b245ddebd8411a08b2a/pandas-profiling-2.2.0.tar.gz (126kB)\n",
      "Requirement already satisfied: pandas>=0.19 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pandas-profiling) (0.24.2)\n",
      "Requirement already satisfied: matplotlib>=1.4 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pandas-profiling) (3.0.3)\n",
      "Requirement already satisfied: jinja2>=2.8 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pandas-profiling) (2.10)\n",
      "Requirement already satisfied: missingno>=0.4.2 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pandas-profiling) (0.4.2)\n",
      "Collecting htmlmin>=0.1.12 (from pandas-profiling)\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/e7/fcd59e12169de19f0131ff2812077f964c6b960e7c09804d30a7bf2ab461/htmlmin-0.1.12.tar.gz\n",
      "Collecting phik>=0.9.8 (from pandas-profiling)\n",
      "  Downloading https://files.pythonhosted.org/packages/45/ad/24a16fa4ba612fb96a3c4bb115a5b9741483f53b66d3d3afd987f20fa227/phik-0.9.8-py3-none-any.whl (606kB)\n",
      "Collecting confuse>=1.0.0 (from pandas-profiling)\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/6f/90e860cba937c174d8b3775729ccc6377eb91f52ad4eeb008e7252a3646d/confuse-1.0.0.tar.gz\n",
      "Requirement already satisfied: astropy in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pandas-profiling) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pandas>=0.19->pandas-profiling) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pandas>=0.19->pandas-profiling) (2018.9)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pandas>=0.19->pandas-profiling) (1.16.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from matplotlib>=1.4->pandas-profiling) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from matplotlib>=1.4->pandas-profiling) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from matplotlib>=1.4->pandas-profiling) (2.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from jinja2>=2.8->pandas-profiling) (1.1.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from missingno>=0.4.2->pandas-profiling) (1.2.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from missingno>=0.4.2->pandas-profiling) (0.9.0)\n",
      "Requirement already satisfied: numba>=0.38.1 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from phik>=0.9.8->pandas-profiling) (0.43.1)\n",
      "Requirement already satisfied: pytest>=4.0.2 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from phik>=0.9.8->pandas-profiling) (4.3.1)\n",
      "Collecting pytest-pylint>=0.13.0 (from phik>=0.9.8->pandas-profiling)\n",
      "  Downloading https://files.pythonhosted.org/packages/64/dc/6f35f114844fb12e38d60c4f3d2441a55baff7043ad4e013777dff55746c/pytest_pylint-0.14.1-py3-none-any.whl\n",
      "Requirement already satisfied: jupyter-client>=5.2.3 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from phik>=0.9.8->pandas-profiling) (5.2.4)\n",
      "Requirement already satisfied: nbconvert>=5.3.1 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from phik>=0.9.8->pandas-profiling) (5.4.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from confuse>=1.0.0->pandas-profiling) (5.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from python-dateutil>=2.5.0->pandas>=0.19->pandas-profiling) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4->pandas-profiling) (40.8.0)\n",
      "Requirement already satisfied: llvmlite>=0.28.0dev0 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from numba>=0.38.1->phik>=0.9.8->pandas-profiling) (0.28.0)\n",
      "Requirement already satisfied: py>=1.5.0 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (1.8.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (19.1.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (1.3.0)\n",
      "Requirement already satisfied: pluggy>=0.7 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.9.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (6.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.4.1)\n",
      "Requirement already satisfied: pylint>=1.4.5 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (2.3.1)\n",
      "Requirement already satisfied: traitlets in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (4.3.2)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (4.4.0)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (18.0.0)\n",
      "Requirement already satisfied: tornado>=4.1 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (6.0.2)\n",
      "Requirement already satisfied: mistune>=0.8.1 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.8.4)\n",
      "Requirement already satisfied: pygments in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (2.3.1)\n",
      "Requirement already satisfied: nbformat>=4.4 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (4.4.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.3)\n",
      "Requirement already satisfied: bleach in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (3.1.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (1.4.2)\n",
      "Requirement already satisfied: testpath in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.4.2)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.5.0)\n",
      "Requirement already satisfied: astroid<3,>=2.2.0 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (2.2.5)\n",
      "Requirement already satisfied: isort<5,>=4.2.5 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (4.3.16)\n",
      "Requirement already satisfied: mccabe<0.7,>=0.6 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (0.6.1)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from traitlets->jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from traitlets->jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (4.4.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from nbformat>=4.4->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (3.0.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from bleach->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.5.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (1.11.1)\n",
      "Collecting typed-ast>=1.3.0; implementation_name == \"cpython\" (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling)\n",
      "  Downloading https://files.pythonhosted.org/packages/47/a1/7a24868c15d84ed7446106d6c3d73807f58232a695452c0a29679e5a1523/typed_ast-1.4.0-cp37-cp37m-win_amd64.whl (155kB)\n",
      "Requirement already satisfied: lazy-object-proxy in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (1.3.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\scminnog15\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.14.11)\n",
      "Building wheels for collected packages: pandas-profiling, htmlmin, confuse\n",
      "  Building wheel for pandas-profiling (setup.py): started\n",
      "  Building wheel for pandas-profiling (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\scminnoG15\\AppData\\Local\\pip\\Cache\\wheels\\c7\\c6\\58\\3a7be8f84fbe64a88f8c3e5cf0360e5abfbdc321d3d421df21\n",
      "  Building wheel for htmlmin (setup.py): started\n",
      "  Building wheel for htmlmin (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\scminnoG15\\AppData\\Local\\pip\\Cache\\wheels\\43\\07\\ac\\7c5a9d708d65247ac1f94066cf1db075540b85716c30255459\n",
      "  Building wheel for confuse (setup.py): started\n",
      "  Building wheel for confuse (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\scminnoG15\\AppData\\Local\\pip\\Cache\\wheels\\b0\\b2\\96\\2074eee7dbf7b7df69d004c9b6ac4e32dad04fb7666cf943bd\n",
      "Successfully built pandas-profiling htmlmin confuse\n",
      "Installing collected packages: htmlmin, pytest-pylint, phik, confuse, pandas-profiling, typed-ast\n",
      "Successfully installed confuse-1.0.0 htmlmin-0.1.12 pandas-profiling-2.2.0 phik-0.9.8 pytest-pylint-0.14.1 typed-ast-1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Path',\n",
       " 'ProfileReport',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__init__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'clean_column_names',\n",
       " 'config',\n",
       " 'controller',\n",
       " 'describe_df',\n",
       " 'display_notebook_iframe',\n",
       " 'get_config_default',\n",
       " 'get_project_root',\n",
       " 'model',\n",
       " 'np',\n",
       " 'pandas_decorator',\n",
       " 'pd',\n",
       " 'rename_index',\n",
       " 'sys',\n",
       " 'templates',\n",
       " 'to_html',\n",
       " 'utils',\n",
       " 'version',\n",
       " 'view',\n",
       " 'warnings']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pandas_profiling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ProfileReport'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ProfileReport.to_html of >"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_profiling.ProfileReport(iris).to_html\n",
    "# export 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/pandas-profiling/pandas-profiling/tree/master/pandas_profiling/view/templates\n",
    "\n",
    "https://github.com/scanny/python-pptx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit의 pip line??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression : 선형성 알고리즘 \n",
    "\n",
    "1.번으로 시용하는 이유 : 선형인지 아닌지 판단하는지 판단하기위해\n",
    "    \n",
    "    선형알고리즘 성능과 비선형알고리즘의 성능이 비슷하면\n",
    "    \n",
    "    선형알고리즘을 적용한다.\n",
    "    \n",
    "    -> 데이터의 선형성을 비교할때 판단할 때 쓴다.\n",
    "    \n",
    "    빅데이터는 선형알고리즘을 잘안한다. 설명가능 여부 판단에서만 쓴다.\n",
    "    \n",
    "    비선형알고리즘에서 어려운 점은 무엇일까?\n",
    "    \n",
    "    -> 복잡하면 복잡할 수록 알아야 되는 정보가 많아진다. \n",
    "    \n",
    "    -> 비선형데이터는 정보가 없으면 성능이 엄청 떨어진다.\n",
    "    \n",
    "    비선형데이터 중에서 뉴럴네트워크는 데이터가 많으면 많을 수록 성능이 좋아진다.\n",
    "    \n",
    "    딥러닝은 기본적으로 데이터가 많아야 한다.\n",
    "    \n",
    "Decision Tree : 수준이 낮다.\n",
    "\n",
    "  - 각각의 자식은 rule 로 바꿀 수 있다. : 시스템 만들기 쉽다. \n",
    "    \n",
    "    통계 : 바이너리 C\n",
    "    \n",
    "    기계 : C42?? 지니, 엔트로피\n",
    "        \n",
    "    오버피팅이 잘 일어나서 가지치기를 한다.\n",
    "    \n",
    "    CART(Classification and regression tree)\n",
    "\n",
    "    > Twoo culture\n",
    "    \n",
    "    random forest\n",
    "    \n",
    "    앙상블 모델의 장점 : 성능이 좋다. 오버피팅이 잘 안난다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ir = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scminnoG15\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\scminnoG15\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ir.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9553571428571429"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score로 정확도 본다\n",
    "Ir.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 작을때 성능이 들쭉 날쭉할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "decision tree / random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "# 지니와 엔트로피 밖에 없다.\n",
    "# 바이너리 분류밖에 안되서 sk에서 트리쓰는것은 별로...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<112x15 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 382 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.decision_path(X_train)\n",
    "# 희소 행렬보는 방법. sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.decision_path(X_train).toaaray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떻게 그려주는지 설명 가능한 얘를 알 수 있다,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.scre(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sg 부스팅은 깔아서 사용해야 한다.\n",
    "\n",
    "feature importance를 체크하는데 decision tree와 random forest를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scminnoG15\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12985259, 0.02897518, 0.31574446, 0.52542777])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.feature_importances_\n",
    "# 피쳐의 중요성을 알려주는 feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴럴네트워크는 kera로 한다.\n",
    "\n",
    "scikit과 keras는 엄청 쉽게 붙는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "# 확률 기반\n",
    "# 가우시안(~NB), : fit, prediciotn, score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -> 사람이 하는 행동처럼 분류하는 얘\n",
    " \n",
    " 어떻게 행동할지에 대한 전략이 있다.\n",
    " \n",
    " 수집한 데이터를 dummy와 비교해서 성능 체크를 먼저한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세상에 특정 데이터에 좋은 알고리즘이 없다!\n",
    "\n",
    "model selection\n",
    "\n",
    "알고리즘, 하이퍼 파라미터 찾아야 된다. -> 노가다\n",
    "\n",
    "scikit은 노가다를 줄여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris =pd.concat([iris,iris_target],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "5                  5.4               3.9                1.7               0.4   \n",
       "6                  4.6               3.4                1.4               0.3   \n",
       "7                  5.0               3.4                1.5               0.2   \n",
       "8                  4.4               2.9                1.4               0.2   \n",
       "9                  4.9               3.1                1.5               0.1   \n",
       "10                 5.4               3.7                1.5               0.2   \n",
       "11                 4.8               3.4                1.6               0.2   \n",
       "12                 4.8               3.0                1.4               0.1   \n",
       "13                 4.3               3.0                1.1               0.1   \n",
       "14                 5.8               4.0                1.2               0.2   \n",
       "15                 5.7               4.4                1.5               0.4   \n",
       "16                 5.4               3.9                1.3               0.4   \n",
       "17                 5.1               3.5                1.4               0.3   \n",
       "18                 5.7               3.8                1.7               0.3   \n",
       "19                 5.1               3.8                1.5               0.3   \n",
       "20                 5.4               3.4                1.7               0.2   \n",
       "21                 5.1               3.7                1.5               0.4   \n",
       "22                 4.6               3.6                1.0               0.2   \n",
       "23                 5.1               3.3                1.7               0.5   \n",
       "24                 4.8               3.4                1.9               0.2   \n",
       "25                 5.0               3.0                1.6               0.2   \n",
       "26                 5.0               3.4                1.6               0.4   \n",
       "27                 5.2               3.5                1.5               0.2   \n",
       "28                 5.2               3.4                1.4               0.2   \n",
       "29                 4.7               3.2                1.6               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "120                6.9               3.2                5.7               2.3   \n",
       "121                5.6               2.8                4.9               2.0   \n",
       "122                7.7               2.8                6.7               2.0   \n",
       "123                6.3               2.7                4.9               1.8   \n",
       "124                6.7               3.3                5.7               2.1   \n",
       "125                7.2               3.2                6.0               1.8   \n",
       "126                6.2               2.8                4.8               1.8   \n",
       "127                6.1               3.0                4.9               1.8   \n",
       "128                6.4               2.8                5.6               2.1   \n",
       "129                7.2               3.0                5.8               1.6   \n",
       "130                7.4               2.8                6.1               1.9   \n",
       "131                7.9               3.8                6.4               2.0   \n",
       "132                6.4               2.8                5.6               2.2   \n",
       "133                6.3               2.8                5.1               1.5   \n",
       "134                6.1               2.6                5.6               1.4   \n",
       "135                7.7               3.0                6.1               2.3   \n",
       "136                6.3               3.4                5.6               2.4   \n",
       "137                6.4               3.1                5.5               1.8   \n",
       "138                6.0               3.0                4.8               1.8   \n",
       "139                6.9               3.1                5.4               2.1   \n",
       "140                6.7               3.1                5.6               2.4   \n",
       "141                6.9               3.1                5.1               2.3   \n",
       "142                5.8               2.7                5.1               1.9   \n",
       "143                6.8               3.2                5.9               2.3   \n",
       "144                6.7               3.3                5.7               2.5   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     target  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "5         0  \n",
       "6         0  \n",
       "7         0  \n",
       "8         0  \n",
       "9         0  \n",
       "10        0  \n",
       "11        0  \n",
       "12        0  \n",
       "13        0  \n",
       "14        0  \n",
       "15        0  \n",
       "16        0  \n",
       "17        0  \n",
       "18        0  \n",
       "19        0  \n",
       "20        0  \n",
       "21        0  \n",
       "22        0  \n",
       "23        0  \n",
       "24        0  \n",
       "25        0  \n",
       "26        0  \n",
       "27        0  \n",
       "28        0  \n",
       "29        0  \n",
       "..      ...  \n",
       "120       2  \n",
       "121       2  \n",
       "122       2  \n",
       "123       2  \n",
       "124       2  \n",
       "125       2  \n",
       "126       2  \n",
       "127       2  \n",
       "128       2  \n",
       "129       2  \n",
       "130       2  \n",
       "131       2  \n",
       "132       2  \n",
       "133       2  \n",
       "134       2  \n",
       "135       2  \n",
       "136       2  \n",
       "137       2  \n",
       "138       2  \n",
       "139       2  \n",
       "140       2  \n",
       "141       2  \n",
       "142       2  \n",
       "143       2  \n",
       "144       2  \n",
       "145       2  \n",
       "146       2  \n",
       "147       2  \n",
       "148       2  \n",
       "149       2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "모델 사용하는 방법\n",
    "\n",
    " 1. 알고리즘 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNeighborsClassifier??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼 파라미터 -> 파라미터에 따라 성능이 달라진다.\n",
    "\n",
    "## *model selection \n",
    "\n",
    " 모델마다 특정 데이터에 성능이 좋은경우가 있다.\n",
    " \n",
    " 데이터가 좋고 많으면 알고리즘별로 성능의 차이가 나지 않는다.\n",
    " \n",
    " -> 세상에 좋은 데이터가 없기 때문에 알고리즘 선택을 잘 한다.\n",
    " \n",
    " feat. peter novig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainning / fit\n",
    "# training data / target data\n",
    "knn.fit??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "knn.fit(X, y)\n",
    "```\n",
    "행렬은 대문자 , 벡터는 소문자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "5                  5.4               3.9                1.7               0.4\n",
       "6                  4.6               3.4                1.4               0.3\n",
       "7                  5.0               3.4                1.5               0.2\n",
       "8                  4.4               2.9                1.4               0.2\n",
       "9                  4.9               3.1                1.5               0.1\n",
       "10                 5.4               3.7                1.5               0.2\n",
       "11                 4.8               3.4                1.6               0.2\n",
       "12                 4.8               3.0                1.4               0.1\n",
       "13                 4.3               3.0                1.1               0.1\n",
       "14                 5.8               4.0                1.2               0.2\n",
       "15                 5.7               4.4                1.5               0.4\n",
       "16                 5.4               3.9                1.3               0.4\n",
       "17                 5.1               3.5                1.4               0.3\n",
       "18                 5.7               3.8                1.7               0.3\n",
       "19                 5.1               3.8                1.5               0.3\n",
       "20                 5.4               3.4                1.7               0.2\n",
       "21                 5.1               3.7                1.5               0.4\n",
       "22                 4.6               3.6                1.0               0.2\n",
       "23                 5.1               3.3                1.7               0.5\n",
       "24                 4.8               3.4                1.9               0.2\n",
       "25                 5.0               3.0                1.6               0.2\n",
       "26                 5.0               3.4                1.6               0.4\n",
       "27                 5.2               3.5                1.5               0.2\n",
       "28                 5.2               3.4                1.4               0.2\n",
       "29                 4.7               3.2                1.6               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "120                6.9               3.2                5.7               2.3\n",
       "121                5.6               2.8                4.9               2.0\n",
       "122                7.7               2.8                6.7               2.0\n",
       "123                6.3               2.7                4.9               1.8\n",
       "124                6.7               3.3                5.7               2.1\n",
       "125                7.2               3.2                6.0               1.8\n",
       "126                6.2               2.8                4.8               1.8\n",
       "127                6.1               3.0                4.9               1.8\n",
       "128                6.4               2.8                5.6               2.1\n",
       "129                7.2               3.0                5.8               1.6\n",
       "130                7.4               2.8                6.1               1.9\n",
       "131                7.9               3.8                6.4               2.0\n",
       "132                6.4               2.8                5.6               2.2\n",
       "133                6.3               2.8                5.1               1.5\n",
       "134                6.1               2.6                5.6               1.4\n",
       "135                7.7               3.0                6.1               2.3\n",
       "136                6.3               3.4                5.6               2.4\n",
       "137                6.4               3.1                5.5               1.8\n",
       "138                6.0               3.0                4.8               1.8\n",
       "139                6.9               3.1                5.4               2.1\n",
       "140                6.7               3.1                5.6               2.4\n",
       "141                6.9               3.1                5.1               2.3\n",
       "142                5.8               2.7                5.1               1.9\n",
       "143                6.8               3.2                5.9               2.3\n",
       "144                6.7               3.3                5.7               2.5\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(iris.iloc[:,:-1],iris[iris.columns[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2 = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 5,\n",
       " 'radius': None,\n",
       " 'algorithm': 'auto',\n",
       " 'leaf_size': 30,\n",
       " 'metric': 'minkowski',\n",
       " 'metric_params': None,\n",
       " 'p': 2,\n",
       " 'n_jobs': None,\n",
       " 'weights': 'uniform'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(knn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn2.fit(iris.iloc[:,:-1],iris[iris.columns[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 5,\n",
       " 'radius': None,\n",
       " 'algorithm': 'auto',\n",
       " 'leaf_size': 30,\n",
       " 'metric': 'minkowski',\n",
       " 'metric_params': None,\n",
       " 'p': 2,\n",
       " 'n_jobs': None,\n",
       " 'weights': 'uniform',\n",
       " 'outputs_2d_': False,\n",
       " 'classes_': array([0, 1, 2]),\n",
       " '_y': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'effective_metric_params_': {},\n",
       " 'effective_metric_': 'euclidean',\n",
       " '_fit_method': 'kd_tree',\n",
       " '_fit_X': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " '_tree': <sklearn.neighbors.kd_tree.KDTree at 0x1abfd802c78>}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(knn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2.predict??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn2.predict([[3,3,3,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[3 3 3 3].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-4f700fcb8a84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mknn2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\classification.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mdata\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \"\"\"\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    550\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[3 3 3 3].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "knn2.predict([3,3,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn2.predict([[3,4,1,2],[3,3,3,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> 2차원 데이터를 무조건 넣어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> 방향 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4]).reshape(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn2.predict([[3,4,1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2.predict_proba??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn2.predict_proba([[3,4,1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.8, 0.2]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn2.predict_proba([[3,3,3,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict_proba 로 디테일보기 : 결정 평면이 1개인 모델은 predict_proba가 없을 수도 있다.\n",
    "\n",
    "0.8 -> k=5중에 4개의 초이쑤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn4 = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# n-jobs : 몇개의 코어 ,성능을 갖고 작업할 것인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn4.fit(iris.iloc[:,:-1],iris[iris.columns[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.66666667, 0.33333333]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn4.predict_proba([[3,3,3,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'auto',\n",
       " 'leaf_size': 30,\n",
       " 'metric': 'minkowski',\n",
       " 'metric_params': None,\n",
       " 'n_jobs': None,\n",
       " 'n_neighbors': 3,\n",
       " 'p': 2,\n",
       " 'weights': 'uniform'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn4.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내가 바꿀 수 있는 하이퍼 파라미터\n",
    "\n",
    "-> 하이퍼파라미터를 조작해서 성능을 조작할 수 있다.\n",
    "\n",
    "성능은 여러가지이다.\n",
    "\n",
    " 1. 정확도\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn5 = KNeighborsClassifier(3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능을 높일 수 있다. 속도를 높인다. \n",
    "\n",
    "-> -1 쓸 수 있는 코어를 다 땡겨쓴다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn5.fit(iris.iloc[:,:-1],iris[iris.columns[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.66666667, 0.33333333]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn5.predict_proba([[3,3,3,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
